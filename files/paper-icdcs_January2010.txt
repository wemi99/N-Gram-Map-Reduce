Empirical Characterization of Uncongested Lambda Networks and 10GbE
Commodity Endpoints
Tudor Marian, Daniel A. Freedman, Hakim Weatherspoon, Ken Birman
Computer Science Department, Cornell University
Itahca, NY-14850, USA
Email: {tudorm,dfreedman,hweather,ken}@cs.cornell.edu

Abstract—High-bandwidth semi-private lambda networks
provisioned with dedicated wavelengths on fiber optic spans
have become increasingly important transport mechanisms for
critical data flows, including scientific, military, and financial users. This work provides a critical examination of the
observed characteristics of such networks in an uncongested
state, evaluating loss, latency, and throughput at the end-host.
Through our measurements, we quantify loss rates and where
loss occurs, thus providing insight into why user communities
often do not receive expected end-to-end performance.
Keywords-Uncongested Lambda Networks; Commodity Endpoints; 10GbE;

I. I NTRODUCTION
Lambda networks have emerged as the enabling technology for a new class of globally distributed high-performance
applications that coordinate across vast geographical distances. Such networking infrastructure provides a highbandwidth semi-private transport mechanism, with dedicated
wavelengths of fiber optic spans that connects remote data
centers and scientific, military, and financial sites. For example, astronomers and physicists at Cornell University often
receive high-volume data streams from the Fermilab or the
Large Hadron Collider, analyze the data at the San Diego
Supercomputer Center, and return results to Cornell for
storage and future reference.
Even though such semi-private lambda networks have
more than ample bandwidth capacity, are dedicated for
specific use, and are routinely idle [1], end-users often do not
receive the performance they expect [2]. This is surprising
given that, unlike the conventional public Internet, semiprivate lambda networks are largely uncongested.
Traffic traveling over uncongested optical networks is
expected to be perfect. Indeed, to break wide-area network speed records, conditions are often nearly perfect
and aggressive, highly-tuned, and specialized software and
hardware are employed [3], [4]. When this is not the case,
performance can suffer. For example, TCP throughput decreases with packet loss. Although the paths are optical fiber,
the end-hosts and routers along the paths are electrical, and
packets can still be lost for various reasons. Interestingly, this
latter example of an imperfect uncongested lambda network
is not uncommon [5].

It is important to measure the characteristics of new
networks as they become available. For example, systematic
measurements have been performed over the ARPANET,
its successor, NSFNET [6], [7], and subsequently over the
early Internet [8]. However, there have been little or no
comprehensive studies of existing lambda networks. More
importantly, studies that have considered a high-bandwidth
optical core [9] did not ask how 10GbE commodity endhosts behave in such an environment. The characteristics of
lambda networks and 10GbE commodity end-hosts need to
be properly measured and understood in order to develop
accurate network models and new protocols best suited for
such an environment.
To this end, we have created a new experimental networking infrastructure testbed, the Cornell National Lambda Rail
(NLR) Rings. We use the testbed to explore the properties
of wide-area optical networks under real use conditions,
focusing on a scenario in which the network user has
semi-dedicated access to an uncongested 10GbE end-to-end
lambda. The Cornell NLR Rings testbed consists of a set
of four variable-length optical network paths that start and
end at Cornell and permit transmission of data on various
cross-country trips. Our longest ring spans nearly 7000 miles
one-way and takes 97.45 ms to traverse in one direction.
Using the Cornell NLR Rings testbed we were able to
confirm that the core of the network is indeed uncongested,
and that loss within the core of the network is very rare.
When we accounted for all packet loss in a 48-hour interval,
we observed only one brief instance where packets were
lost in the core. In contrast, we find that substantial levels
of loss occur at the end-host. This is problematic given
that endpoints are typically 10GbE commodity machines
with default configurations and stocked with out-of-thebox conventional transport protocols such as vanilla TCP—
known to perform poorly in high bandwidth-delay product
environments [10], [11].
Importantly, we show that end-to-end measurements are
highly dependent on the configuration of the end-host. In
particular, we show that:
• The size of the socket buffer and of the DMA (direct
memory access) ring determines the loss rate experienced by the end-host.

The TCP throughput decreases as packet loss increases,
and this phenomenon grows in severity as a function
of both the path length and the window size. The
congestion control algorithm turns out to have only a
marginal role in determining the achievable throughput.
• The NIC (Network Interface Controller) interrupt
affinity—i.e. the policy that determines which
CPU / core handles which interrupt issued by the
network card—determines the ability of the end-host to
handle load graciously or not (e.g. the packet receive
rate where the host enters receive livelock [12]). It
also determines the amount of loss observed at the
end-host.
• Packet batching, performed by kernel NAPI [13] and
the NIC interrupt throttling, increases overall throughput. However, such techniques are detrimental for
latency-sensitive measurements, for example interfering
with the measurement of packet inter-arrival times.
The paper is structured as follows: In Section II we
detail two examples of uncongested lambda networks—the
TeraGrid [14] and our own Cornell NLR Rings testbed.
Next we discuss the limitations of the traffic monitoring
infrastructure and available measurements for the TeraGrid
network. In contrast, we were successful in using the Cornell
NLR Rings testbed to perform high-resolution experimental
measurements with which we characterize the behavior of an
uncongested lambda network (10GbE) end-host. Section III
presents our experimental results, and Section IV presents
related work, while Section V concludes.
•

II. U NCONGESTED L AMBDA N ETWORKS
Lambda networking, as defined by the telecommunications industry, is the technology and set of services directly
surrounding the use of multiple optical wavelengths to
provide independent communication channels along a strand
of fiber optic cable [15]. In this section, we present two
examples of lambda networks, namely TeraGrid [14] and the
Cornell NLR Rings testbed. Both networks consist of semiprivate, uncongested 10 Gigabit per second (Gbps) optical
Dense Wavelength Division Multiplexing (DWDM) or OC192 Synchronous Optical Networking (SONET) links.
A. TeraGrid
TeraGrid [14] is an optical network interconnecting ten
major supercomputing sites throughout the United States.
The backbone provides 30Gbps or 40Gbps aggregated
throughput over 10GbE and SONET OC-192 links [9]. Endhosts, however, connect to the backbone via 1Gbps links,
hence the link capacity between each pair of end-host sites
is 1Gbps.
Of particular interest is the TeraGrid monitoring framework [5]; each of the ten sites perform periodic UDP measurements with the Iperf [16] application, reporting throughput and loss rates. Every site issues a 60 second probe to

30

% of Measurements

25
20
15
10
5
0
0.01 0.03 0.05 0.07 0.1 0.3 0.5
% of Lost Packets
Figure 1.

0.7

1

Loss Rates on TeraGrid.

every other site once an hour, resulting in a total of 90
overall measurements collected every hour. Figure 1 shows a
histogram of percentage packet loss between November 1st,
2007, and January 25th, 2008, where 24% of the measured
loss rates had 0.01% loss and a surprising 14% of them had
0.10% loss. After eliminating a single TeraGrid site (Indiana
University) that dropped incoming packets at a steady 0.44%
rate, 14% of the remainder of the measurements showed
0.01% loss, while 3% showed 0.10% loss.
Although small, such numbers are sufficient to severely
reduce the throughput of TCP/IP on these high-latency, highbandwidth paths [11], [17]. Conventional wisdom states that
optical links do not drop packets. Indeed, carrier-grade optical equipment is often configured to shut down beyond bit
error rates of 10−12 —or one out of a trillion bits. However,
the reliability of the lambda network is far less than the
sum of its optical parts—in fact it can be less reliable by
orders of magnitude. Consequently, applications depending
on protocols like TCP/IP, which require high reliability from
high-speed networks, may be subject to unexpectedly high
loss rates, and hence low throughput.
Note that Figure 1 shows the loss rate experienced during
UDP traffic on end-to-end paths, which is not easily generalized to TCP. Furthermore, it is unclear if packets were
dropped along the optical path, at intermediate devices (e.g.
optical or electrical switches), or at the end-hosts. Finally,
loss occurred on paths where levels of optical link utilization
(determined by 20-second moving averages) were consistently lower than 20%, making congestion highly unlikely,
a conclusion supported by the network administrators [18].
Lacking more detailed information about the specific
events that trigger loss in TeraGrid, we arrive at an impasse:
One can only speculate about the sources of the high
observed loss rates. Several hypotheses suggest themselves:
• Device clutter—the critical communication path between any two end-hosts consists of many electronic

NLR(NYC)
Backbone
Router
long
medium

exception

short
Cornell
(Ithaca)
Backbone
Router

~350km
Cornell (NYC)
Backbone
Router

Egress
R900 End-host

Figure 2.

•

•

Ingress
End-host R900

Cornell NLR Rings Topology.

devices, each of which represents a potential point of
failure.
End-host loss—common wisdom has that the majority
of packets are dropped when incoming traffic overruns
the receiving machine. 1
Cost-benefit of service—It may be the case that loss
rates are typical of any large-scale networks, where
the cost of immediately detecting and fixing failures
is prohibitively high. For example, dialogue with the
administrators revealed that the steady loss rate experienced by Indiana University site was due to a faulty
network card, and the measurements showed that the
error persisted over at least a three month period.

B. Cornell University NLR Rings
Instead of probing further into the characteristics of the
TeraGrid network, we chose to create our own network
measurement testbed here at Cornell: the Cornell National
LambdaRail (NLR) Rings testbed. In order to understand the
properties of the Cornell NLR Rings, we provide a fairly
detailed description of our measurement infrastructure in
this section. The following section contains experiments and
measurement results.
The Cornell NLR Rings testbed takes advantage of the
existing National LambdaRail [19] backbone infrastructure.
Two commodity end-hosts are connected to the backbone
router in Ithaca, New York, shown as Egress and Ingress
1 In NAPI [13] mode, packets can be dropped in either of two places:
when there is insufficient room available on the rx DMA ring, and when
enqueueing the packet on the socket buffer would breach the socket buffer
limit. In both cases the receiver is overwhelmed and loss is observed, but
the precise conditions capable of producing these losses differ.

end-hosts in Figure 2. The commodity end-hosts are fourway Xeon quad-core Dell PowerEdge R900 machines with
32GB RAM, each equipped with an Intel 10GbE LR PCIe
x8 adapters (EXPX9501AFXLR). They run a preemptive
64-bit Linux 2.6.24 kernel, with the Intel ixgbe driver
version 1.3.47. The generic segmentation offload (GSO) was
disabled since it is incompatible with the Linux kernel packet
forwarding subsystem.
Through a combination of VLAN (virtual LAN) tagging
(IEEE 802.1Q) and source-, policy-, and destination-based
routing we established four static 10GbE full duplex routes
that begin and end at Cornell, but transit various physical
lengths: a short ring to New York City and back, an
exception ring via Chicago, Atlanta, and New York City,
a medium ring via Chicago, Denver, Houston, Atlanta, and
New York City, and a long ring through Chicago, Denver,
Seattle, Los Angeles, Houston, Atlanta, and New York City
(Figure 2). The one-way latency (one trip around the ring)
is 7.95 ms for the short path, 37.3 ms for the exception path,
68.9 ms for the medium path, and 97.45 ms for the long path,
respectively. All optical point-to-point backbone links use
Dense Wavelength Division Multiplexing (DWDM) technology, except for a single OC-192 SONET link between
Chicago and Atlanta, thus differentiating the exception path
from all others. Overall, the Cornell NLR Rings use 10 out
of the total 13 layer three NLR links.
The Cornell (Ithaca and NYC) Backbone Routers and the
NLR (NYC) Backbone Router handle double the amount
of aggregated traffic of any other router along the paths, but
that the traffic flows in distinct directions. All three are highend dedicated Cisco hardware. The NLR (NYC) Backbone
Router is a CRS-1 model, while the Cornell (Ithaca) and
Cornell (NYC) Backbone Routers are Catalyst 6500 series
hardware. These routers all have sufficient backplane to
operate at their full rated speed of 10Gbps irrespective of
the traffic pattern; the loads generated in our experiments
thus far have provided no evidence to the contrary.
The Cornell (Ithaca and NYC) Catalyst 6500s are
equipped with WSX6704-10GE Ethernet modules with centralized forwarding cards. The Quality of Service (QoS)
feature of these modules was disabled, hence in the event
of an over-run, all traffic is equally likely to be discarded.
In particular, packets are served in the order in which they
are received. If the buffer is full, all subsequent packets
are dropped, a discipline sometimes referred to as FIFO
queueing with drop-tail [20].
Figure 3 shows the layer-3 load on the entire NLR backbone network at the time we performed controlled 2Gbps
high-bandwidth UDP traffic experiments over the exception
and the long paths. The figure highlights the topological
setup chosen for the exception and long paths. Importantly,
it also shows that the paths are uncongested. When our tests
are running, the associated loop will be shown in yellow: a
level of link utilization roughly 20% exclusively on the path

Figure 3.

Exception (left) and long (right) path experiment loads, as observed by the Global NOC Realtime Atlas.

segments part of our Cornell NLR Rings testbed.

sockbuf_loss
rx_ring_loss
network_loss

0.0006

In this section, we use the Cornell NLR Rings testbed
to answer the following questions with respect to the traffic
characteristics over uncongested lambda networks:
•
•
•

0.0003
0.0002
0.0001
2400
2000
1600
1200
800
400

0

2400
2000
1600
1200
800
400

Our experiments generate UDP and TCP Iperf [16] traffic
between the two commodity end-hosts over all paths, i.e.
between the Egress and the Ingress end-hosts depicted in
Figure 2. We modified Iperf to report (for UDP traffic) precisely which packets were lost and which were received out
of order. Additionally, for increased precision, we patched
the Linux kernel to time-stamp each received packet with
the TSC (CPU time stamp counter that counts clock cycles)
instead of the monotonic wall-clock. We implement this by
overwriting the SO_TIMESTAMPNS socket option to return
the 64 bit value of the TSC register. For the TSC timestamp values to be monotonic (a validity requirement), they
must originate from the same CPU. This means that all NIC
interrupts notifying a packet arrival must be handled by the
same CPU, since received packets are time-stamped in the
interrupt service routine. Before and after every experimental
run, we read kernel counters on both sender and receiver
that account for packets being dropped at the end-host in
the DMA ring, socket buffer, or TCP window. The default
size of each rx (receive) and (transmit) tx DMA ring is 1000
slots, while the MTU (Maximum Transfer Unit) is set to the
default 1500 bytes (i.e. we did not use jumbo frames).
Throughout our experiments all NLR network segments
were uncongested—as a matter of fact the background traffic

0.0004

2400
2000
1600
1200
800
400

A. Experimental Setup

0.0005

2400
2000
1600
1200
800
400

•

Under what conditions does packet loss occur, and
where does the loss take place?
How does NIC interrupt affinity affect packet loss?
How does the choice of TCP protocol stack impact the
measured throughput?
How does packet batching, induced by NAPI [13]
and / or NIC interrupt throttling, affect overall throughput and latency measurements, like packet dispersion?

Packet loss fraction

III. E XPERIMENTAL M EASUREMENTS

7.95ms
(short)

37.3ms
(exception)

68.9ms
(medium)

97.45ms
(long)

Figure 4. UDP loss ratio, 1MB socket buffers, irqbalance on, sender data
rates between 400 and 2400Mbps.

over each link never exceeded 5% utilization.

2

B. Packet Loss
To measure packet loss over the Cornell NLR Rings
testbed, we performed many sequences of 60-second UDP
Iperf runs over a period of 48 hours. We iterated over
all paths (short, exception, medium, and long), for data
rates ranging between 400Mbps to 2400Mbps at 400Mbps
intervals. The sender and receiver were configured identically as follows. First, they were allocated socket buffers
of 1, 2, or 4MB. Next, we alternated between using
the irqbalance [21] daemon and statically assigning
the interrupts issued by the NICs to specific CPUs. The
irqbalance daemon uses the kernel CPU affinity interface (the /proc/irq/IRQ#/smp_affinity special
files) and periodically re-assigns hardware interrupts across
processors in order to increase performance. We present
results first using interrupts via irqbalance, then using NIC
interrupt affinity to bind interrupts to a single CPU.
2 Utilization

is computed by the monitoring system [1] every 1-5 seconds.

sockbuf_loss
rx_ring_loss
network_loss

0.0006
Packet loss fraction

Packet loss fraction

0.2
0.15
0.1

0.05

0.0005
0.0004
0.0003
0.0002
0.0001

97.45ms
(long)

(a)
Figure 5.

7.95ms
(short)

37.3ms
(exception)

68.9ms
(medium)

97.45ms
(long)

(b) Y-axis scaled as that of Figure 4.

UDP loss ratio, 1MB socket buffers, irqs bound to cores, sender data rates between 400 and 2400Mbps.

sockbuf_loss
rx_ring_loss
network_loss

0.0006
Packet loss fraction

0.2
Packet loss fraction

2400
2000
1600
1200
800
400

2400
2000
1600
1200
800
400

68.9ms
(medium)

2400
2000
1600
1200
800
400

2400
2000
1600
1200
800
400

37.3ms
(exception)

2400
2000
1600
1200
800
400

2400
2000
1600
1200
800
400

7.95ms
(short)

0

2400
2000
1600
1200
800
400

2400
2000
1600
1200
800
400

0

0.15
0.1

0.05

0.0005
0.0004
0.0003
0.0002
0.0001

97.45ms
(long)

(a)
Figure 6.

7.95ms
(short)

37.3ms
(exception)

68.9ms
(medium)

97.45ms
(long)

(b) Y-axis scaled as that of Figure 4.

UDP loss ratio, 4MB socket buffers, irqs bound to cores, sender data rates between 400 and 2400Mbps.

Interrupts via Irqbalance
Figure 4 shows fractional packet loss versus sender data
rate for each of the path lengths with the irqbalance
daemon running and the socket buffer size set to 1MB. The
bars represent total loss as observed by the receiving Iperf
application, split into three components denoting the precise
location where loss can occur. As seen in the Figure, loss
may be a consequence of over-running the socket buffer
(sockbuf_loss), over-running the rx (receive) DMA ring
(rx_ring_loss), or numerous factors within the network
core (network_loss). 3 The Figure shows that there was
zero loss in the network core; all loss occurs within the
receivers’ socket buffer. Moreover, for socket buffer sizes
of 2MB and 4MB respectively, there is zero loss reported
3 The

2400
2000
1600
1200
800
400

2400
2000
1600
1200
800
400

68.9ms
(medium)

2400
2000
1600
1200
800
400

2400
2000
1600
1200
800
400

37.3ms
(exception)

2400
2000
1600
1200
800
400

2400
2000
1600
1200
800
400

7.95ms
(short)

0

2400
2000
1600
1200
800
400

2400
2000
1600
1200
800
400

0

ixgbe NIC driver was built with NAPI support, hence there is no
backlog queue in the kernel between the DMA ring and the socket buffer.

for the tested data rates—hence the lack of corresponding
figures.
There are other places not in the network core where
loss may have occurred; however, we dismiss them for the
following reasons:
•

•
•

•

The sender socket buffer is never over-run during the
entire 48-hour duration of the experiment—in accordance with the blocking nature of the socket API.
The sender (transmit) tx DMA ring was never over-run
during the entire 48-hour duration of the experiment.
Neither the sender nor receiver NIC report any errors
(e.g. corruption) or internal (on board) buffer over-run
during the entire 48-hour duration of the experiment.
Since we used Iperf with UDP traffic, the receiver does
not transmit any packets.

Interrupts Bound to a Single CPU

Packet Loss Summary
In general, these experiments show virtually no loss in the
network core. Instead, loss occurs at the end-hosts, notably
at the receiver. End-host loss is typically the result of a buffer
over-run in the socket, backlog queue, or DMA ring.
4 We have experienced path blackouts due to various path segments being
serviced, replaced, or upgraded.

reno
cubic
bic
vegas
htcp
hybla
illinois
scalable
westwood
yeah

5000
Throughput (Mbps)

A more interesting scenario emerges when we assign all
interrupts from the NIC to be handled by a single core.
In particular, Figures 5 and 6 show fractional UDP packet
loss for socket buffer sizes of 1MB and 4MB. For a 2MB
socket buffer the plot is identical in shape to Figure 5, but
with a ∼ 0.12 packet loss fraction for a sender data rate of
2400Mbps.
There are three key points of interest. First, Figure 6
shows a particular event—the only loss in the core network
we experienced during the entire 48-hour period, occurring
on the medium path (one way latency is 68.9 ms) for a
sender data rate of 400Mbps. During the course of the
experiments, this was a single outlier that occurred during a
single 60-second run. We believe it could have been caused
by events such as NLR maintenance. 4
Second, at 2400Mbps there is an abrupt increase in
observed loss. Taking a closer look, we noticed that the
receiver was experiencing receive livelock [12]. On a 2.6
Linux kernel, receive livelock can be easily observed as the
network bottom half executing in a softirq cannot finish in a
timely manner, and is forced to start the the corresponding
ksoftirqd/CPU# kernel thread. The thread runs exclusively on the same CPU, and picks up the remaining work
the softirq did not finish, acting as a rate limiter. As a
result, the receive livelock occurs given that all interrupts
(rx, tx, rxnobuff, etc.) were serviced by a single overwhelmed CPU—the same CPU that runs the corresponding
ksoftirqd/CPU# and the user-mode Iperf task. The Iperf
task is placed on the same CPU since the scheduler’s default
behavior is to minimize cache thrashing. Consequently, there
is not enough CPU time remaining to consume the packets
pending in the socket buffer in a timely fashion. Hence,
the bigger the socket buffer, the more significant the loss,
precisely as Figures 5(a) and 6(a) show.
Third, the end-host packet loss increases with the sender
data rate, shown in Figures 5(b) and 6(b). Figure 5(b)
corresponds to a relatively small buffer, 1MB, so the effect
is clear. Figure 6(b) uses a larger buffer (4MB) hence no
packets were lost for data rates below 2400Mbps except
for negligible loss along the short path at a data rate
of 2000Mbps. Similarly, this trend is evident in Figure 4
(irqbalance enabled); however, at sufficiently high data rates,
irqbalance spreads the interrupts to many different CPUs and
the loss decreases.

4000
3000
2000
1000
0
7.95
(short)

Figure 7.

37.3
68.9
(exception) (medium)
One Way Latency (ms)

97.45
(long)

TCP throughput for a single flow configured for 1Gbps.

NIC interrupt affinity to CPUs affects loss, and is pivotal in determining the end-host’s ability to handle load
graciously. Our experiments show that at high data rates
irqbalance works well (i.e. decreases loss), whereas at low
data rates, binding NIC interrupts to the same CPU reduces
loss more than irqbalance. One benefit of binding all NIC
interrupts to the same CPU stems from the fact that the driver
(code and data), the kernel network stack, and the user-mode
application incur less CPU cache pollution overhead.
Unless the receiver is overloaded, a sufficiently large
socket buffer prevents loss. For example, in the case of
the interrupts via irqbalance loss does not occur for buffer
sizes of 2 and 4MB respectively. Likewise, Figure 6(b)
shows that, for data rates below 2400Mbps, only the short
path at 2000Mbps exhibited end-host loss (specifically,
0.000002647) for a 4MB socket buffer.
C. Throughput
Although UDP is well suited for measuring packet loss
rates and indicating where loss occurs, TCP [22] is the defacto reliable communication protocol; it is virtually embedded in every operating system’s network stack. Many TCP
congestion control algorithms have been proposed—Fast
TCP, High Speed TCP, H-TCP, BIC, CUBIC, Hybla, TCPIllinois, Westwood, Compound TCP, Scalable TCP, YeAHTCP—and almost all have features intended to improve
performance over high bandwidth high latency links, i.e.
Long Fat Networks (LFNs).
To measure the achievable throughput, we conducted a
set of 24-hour bulk TCP transfer tests over all the paths
of the Cornell NLR Rings, using 60 second Iperf bursts.
Throughout our experiments we used all variants enumerated
above, except for TCP Low Priority, and TCP Veno—the
latter is an optimized variant for wireless networks and
designed to better deal with random loss.
Figure 7 shows the TCP throughput for a single flow

Throughput (Mbps)

5000
4000
3000
2000

Packet loss fraction (normalized)

0.0014

reno
cubic
bic
vegas
htcp
hybla
illinois
scalable
westwood
yeah

0.0012
0.001
0.0008

snd_txloss
snd_rxloss
snd_tcploss
snd_tcppruning
rcv_txloss
rcv_rxloss
rcv_tcploss
rcv_tcppruning

0.0006
0.0004
0.0002

1000
0

7.95
(short)

37.3
68.9
(exception) (medium)
One Way Latency (ms)

97.45
(long)

(a) TCP throughput.
Figure 8.

yeah
westwood
scalable
illinois
hybla
htcp
vegas
bic
cubic
reno
yeah
westwood
scalable
illinois
hybla
htcp
vegas
bic
cubic
reno
yeah
westwood
scalable
illinois
hybla
htcp
vegas
bic
cubic
reno
yeah
westwood
scalable
illinois
hybla
htcp
vegas
bic
cubic
reno

0

7.95ms
(short)

37.3ms
(exception)

68.9ms
(medium)

97.45ms
(long)

(b) TCP loss.
TCP throughput (a) and loss (b) for four concurrent flows.

for all available TCP versions, over all paths. The TCP
window sizes were configured with respect to each path
RTT (Round Trip Time) such that a single flow would be
able to reach 1Gbps. A higher window translates into larger
amount of in-flight and not yet acknowledged data, which is
necessary but not sufficient—as the figures show—to yield
high throughput on high latency, high bandwidth links. In
particular, a single TCP flow of 1Gbps on the short path
should have a TCP window no smaller than 2MB, 9.4MB
on the exception path, 17.3MB on the medium path, and
24.4MB on the long path. Almost all TCP variants yield
roughly the same throughput with the exception of TCP
Vegas that underperforms. No packet loss occurred for any
of the single-flow TCP variants.
Since the TCP window size is a kernel configuration
parameter and superuser privileges are required to adjust it,
typical user-mode applications like GridFTP [10] strive to
maximize throughput by issuing multiple TCP flows in parallel to fetch / send data. To experiment with multiple flows,
we ran four TCP Iperf flows in parallel in order to saturate
each end-host’s capacity and yield maximum throughput.
Figure 8(a) depicts the throughput results. Although the
window sizes should be sufficient, the overall throughput decreases as the path length increases. Importantly, loss at the
end-host does occur for multiple TCP flows. Moreover, some
TCP variants yield marginally better aggregate throughput
than others when competing with flows of the same type.
The TCP throughput over the short path is identical to the
maximum throughput achieved during control experiments.
The control experiments were performed by connecting the
end-hosts directly with an optical patch cable.
Even though TCP is a reliable transport protocol, packet
loss, albeit at the end-host, does affect performance [17].
Figure 8(b) shows the fractional packet loss corresponding
to the TCP throughput in Figure 8(a). Unlike UDP loss,

any analysis of TCP loss must account for retransmissions,
selective and cumulative acknowledgments, different size of
acknowledgments, and timeouts. Figure 8(b) shows loss as
reported both at the sender (bar names starting with snd_)
and the receiver (bar names starting with rcv_), within the
DMA rings (bar names ending in _txloss and _rxloss),
inferred by TCP itself (bar names ending in _tcploss),
and due to the inability of the user-mode process owning
the socket to read the data in a timely fashion (bar names
ending in _tcppruning).
Although at first glance the figure appears overly dense,
there are only three points where loss is occurring: the
receiver’s rx (receive) DMA ring (rcv_rxloss), loss
that is then largely inferred by the sender’s TCP stack
(snd_tcploss), and finally, within the sender’s rx (receive) DMA ring (snd_rxloss). The sender sends MTU
size (1500 bytes) TCP data packets and receives TCP empty
(52 byte) payload ACKs (20 byte IP header + 20 byte TCP
header + 12 byte TCP options).
There are two significant observations. First, loss occurs
at the end-host in the rx DMA rings—the receiver will
drop inbound payload packets, while the sender will drop
inbound ACK packets. Recall that the NIC is configured to
a default value of 1000 slots per DMA ring. The socket
buffer is essentially the TCP window; hence, it is adjusted
to a large value in this experiment. Second, there are far
more ACK packets (snd_rxloss) being lost than payload
packets (rcv_rxloss). Figure 8 (b) shows the loss fraction
normalized to packet size instead of packet count. However,
since ACKs are cumulative, TCP can afford to lose a
significant portion of a window worth of ACKs on the rx
DMA ring, provided that a single ACK with an appropriate
(subsequent) sequence number is delivered to the TCP stack.
There is no loss observed by TCP Vegas since its low
throughput is insufficient to induce end-host loss, a scenario

identical to the one already described in Figure 7.
Our experiments show that as path length increases, more
data and, importantly, more ACKs are lost since the TCP
windows are enlarged to match the bandwidth delay product
of the longer paths. This affects performance, and throughput
decreases as the path length increases.
D. Packet Batching
In this section, we examine more closely the impact of
receiver livelock on the measurements reported above.
A CPU is notified of the arrival and departure of packets
at a NIC by interrupts [23]. The typical interrupt-driven
commodity kernel, however, can find itself driven into a
state in which the CPU expends all available cycles processing interrupts, instead of consuming received data. If
the interrupt processing overhead is larger than the rate at
which packets arrive, receive livelock [12] will occur. 5 The
typical solution is to batch packets by parameterizing the
NIC to generate a single interrupt for a group of packets that
arrive during some specified time interval. For example, Intel
NICs offer an Interrupt Throttling configuration parameter
that limits the maximum number of interrupts issued per
second. If the device supports it, the kernel can take it one
step further by functioning in “New API” (NAPI) [13] mode.
Instead of being exclusively interrupt-driven, a NAPI kernel
is interrupt-driven at low data rates, but switches to polling
at high data rates.
The benefit of packet batching techniques is an increase in
maximum achievable bandwidth for a particular commodity
architecture. For example, with NAPI and Interrupt Throttling disabled, the maximum achievable TCP throughput on
our setup was approximately 1.9Gbps in control experiments
with end-hosts directly connected to each other. With NAPI
enabled and Interrupt Throttling rate set to the default
value, we achieved around 3Gbps throughput, as shown in
Figure 8(a). Note that, by default, the interrupt throttling rate
parameter limits the interrupts issued per second to no more
than 8000.
However, this does not mean that packet batching is ideal
in all scenarios, even though vanilla kernels and drivers
enable batching by default. To illustrate this, consider a
standard metric provided by high-end Ethernet test products [24]—the packet inter-arrival time, also known as
packet dispersion. To perform this type of measurements on
the Cornell NLR Rings testbed, we patched the Linux kernel
to timestamp every received packet as early as possible (in
the driver’s top half interrupt service routine) with the TSC
(CPU time stamp counter) instead of the monotonic kernel
wall-clock, thereby achieving cycle (nanosecond) resolution.
Figure 9 shows the packet inter-arrival time for a UDP
Iperf experiment consisting of a sequence of 300 packets at
5 The interrupt overhead consists of two context switches and executing
the top half of the interrupt service routine.

a data rate of 100Mbps (about one packet every 120 µs) with
and without Interrupt Throttling enabled and NAPI disabled.
Figure 9(b) shows that the interrupt batching superimposes
an artificial structure on top of the packet dispersion, thereby
yielding spurious measurement results. The implications
of this phenomenon have significant consequences. For
example, tools that rely on accurate packet inter-arrival
measurements to estimate capacity and available bandwidth
yield meaningless results when employed in conjunction
with packet batching.
E. Summary of Results
Our experiments answered two general questions with
respect to uncongested lambda network traffic. First, we
showed that loss occurs almost exclusively at the end-hosts
as opposed to within the network core, typically a result
of the receiver being over-run. Second, we showed that
measurements are extremely sensitive to the configuration
of the commodity end-hosts. In particular, we show that:
• Socket buffer size and DMA ring size determine the
amount of loss experienced.
• TCP throughput decreases with the increase in packet
(and TCP ACK) loss, with the increase in path length,
and the increase in window size. The congestion control
algorithm is only marginally important in determining
the achievable throughput (i.e. most variants are similar).
• NIC interrupt affinity is pivotal in how the end-host
handles load, and determines the amount of loss observed at the end-host.
• Built-in kernel NAPI and NIC interrupt throttling improve throughput, although they are detrimental for
latency sensitive measurements. This reinforces the
conventional wisdom that there is no “one-size-fits-all”
set of parameters, and careful parameter selection is
necessary for the task at hand.
IV. R ELATED W ORK
There has been a tremendous amount of work aimed at
characterizing the Internet at large by analytical modeling,
simulation, and empirical measurements. Measurements, in
particular, have covered a broad range of metrics, from
end-to-end packet delay and packet loss behavior [25],
[26], to packet dispersion (spacing) experienced by backto-back packets [27], packet inter-arrival time [7], per-hop
and end-to-end capacity, end-to-end available bandwidth,
bulk transfer capacity, achievable TCP throughput, and other
general traffic characteristics [6]. However, there has been
little work aimed at characterizing uncongested semi-private
or dedicated networks, like modern optical lambda networks.
The need for instruments with which to perform such
measurements has led to the development of a myriad of
tools [27], [28], [29], [30], [31], [32]. These tools are
typically deployed in an end-to-end fashion for convenience,

160

Measured at the receiver
Intended at the sender

140

Packet Inter-Arrival Time (us)

Packet Inter-Arrival Time (us)

160

120
100
80
60
40
20

Measured at the receiver
Intended at the sender

140
120
100
80
60
40
20

0

0
0

50

100

150
200
Packet Number

250

300

(a) Interrupt throttling disabled at the receiver.
Figure 9.

0

50

100

150
200
Packet Number

250

300

(b) Interrupt throttling enabled at the receiver.

Packet Inter-Arrival Time, NAPI off and interrupt throttling disabled (a), enabled (b) (at the receiver).

and often embody a tradeoff between intrusiveness and
accuracy [33]. For example, some tools rely on self-induced
congestion, while others rely on relatively small probes
consisting of packet pairs or packet trains. Tools like these
have have become essential and provide a solid foundation
for measurements; for example, we have saved significant
time by working with (and extending) the existing Iperf [16].
Not surprisingly, Internet measurements provide a snapshot of the characteristics of the network at the time the
measurements are performed. For example, in its early days,
the Internet was prone to erratic packet loss, duplication,
reordering, and the round trip transit delays were observed
to vary over a wide range of values [8]. Today, none of these
issues remains, although other challenges have emerged.
Historically, networks have been characterized as they
became available—ARPANET, its successor, NSFNET [6],
[7], and the early Internet [8] have all been the focus of
systematic measurements. Murray et. al. [9] compare endto-end bandwidth measurement tools on the 10GbE TeraGrid
backbone, while Bullot et. al. [34] evaluated the throughput
of various TCP variants by means of the standard Iperf,
over high speed long distance production networks of the
time (from SLAC to Caltech, to University of Florida, and
to University of Manchester over OC-12 links of maximum
throughput of 622Mbps)—similar to the experiments in
Section III-C.
However, unlike our experiments, Bullot et. al. [34] focused on throughput and related metrics, like the stability
(in terms of throughput oscillations), and TCP behavior
while competing against a sinusoidal UDP stream. Although
disregarding loss patterns and end-host behavior, the authors
do provide insight into how the txqueuelen parameter
(i.e. the length of the backlog queue residing between the
IP layer and the DMA rx ring—made obsolete by NAPI)
affects throughput stability. In particular, larger values of

the txqueuelen are correlated with more instability. An
equally interesting observation is that reverse-cross-traffic
affects some TCP variants more than others, since they
alter ACK delivery patterns (e.g. ACK compression due to
queueing or loss). It is also worth noting that the authors
perform a set of tentative TCP performance measurements
on 10Gbps links, using jumbo (MTU of 9000 bytes) frames.
By contrast, there has been relatively little work that
investigates the effect of traffic patterns on the end-host,
and the end-host’s ability to handle such traffic, especially
for uncongested lambda networks with 10GbE end-hosts.
Mogul et. al. [12] investigated the effect of high date rate
traffic on the end-host, noting that a machine would live-lock
and spend all available cycles while handling the interrupt
service routine as a result of packets being received, only
to drop these packets at the subsequent layer, and hence
fail to make forward progress. Consequently, NAPI [13] and
NIC on-board Interrupt Throttling have been widely adopted
techniques to the point where they are enabled by default
in vanilla kernels. On the other hand, an interesting study
looked at how “interrupt coalescence” (produced by NAPI
possibly in conjunction with Interrupt Throttling) hinders
active measurement tools that rely on accurately estimating
packet dispersion to measure capacity and available bandwidth [35]. Since the packets were time-stamped in userspace, context switches at the receiver cause similar behavior
as packet batching.
V. C ONCLUSION
We perform an empirical study of end-to-end packet loss
and throughput as observed on end-hosts connected by highbandwidth, semi-private, uncongested lambda networks—an
increasingly important transport medium for critical data
flows, including those of scientific, military, and financial
communities. We have architected the Cornell NLR Rings

testbed to provide insight into this emerging type of networks. In particular, we observe that there is virtually no
loss in the network core as it is indeed uncongested most of
the time. By contrast, when loss occurs, it does so at the endhost, typically as a result of a buffer over-run. Importantly,
we show that measurements are highly dependent on endhost configuration, like socket buffer size, TCP window size,
NIC interrupt affinity, NAPI, and interrupt throttling, and
there is no “one-size-fits-all” set of parameters.
R EFERENCES
[1] “NLR PacketNet Atlas,” http://atlas.grnoc.iu.edu/atlas.cgi?
map name=NLR%20Layer3, 2009.
[2] “David A. Lifka, Director, “The Center for Advanced Computing. Cornell Univ. Private Communication”,” Dec 2008.
[3] “Internet2 Land Speed Record,” http://www.internet2.edu/lsr/.
[4] S. C. Simms, G. G. Pike, and D. Balog, “Wide Area Filesystem Performance using Lustre on the TeraGrid,” in Teragrid
Conference, 2007.
[5] “TeraGrid Performance Monitoring (Iperf and RTT),” https:
//network.teragrid.org/tgperf/, 2005.
[6] K. Claffy, G. C. Polyzos, and H. Braun, “Traffic Characteristics of the T1 NSFNET Backbone,” in INFOCOM ’93.
[7] S. A. Heimlich, “Traffic characterization of the NSFNET national backbone,” SIGMETRICS Perform. Eval. Rev., vol. 18,
no. 1, pp. 257–258, 1990.
[8] D. Sanghi, A. K. Agrawala, O. Gudmundsson, and B. N.
Jain, “Experimental Assessment of End-to-end Behavior on
Internet,” in Proc. IEEE INFOCOM ’93, 1993, pp. 867–874.
[9] M. Murray, S. Smallen, O. Khalili, and M. Swany, “Comparison of End-to-End Bandwidth Measurement Tools on the
10GigE TeraGrid Backbone,” in Proceedings of GRID ’05.
[10] W. Allcock, J. Bester, J. Bresnahan, A. Chervenak, L. Liming,
and S. Tuecke, “GridFTP: Protocol extensions to FTP for the
Grid,” GGF Document Series GFD, vol. 20, 2003.
[11] M. Balakrishnan, T. Marian, K. Birman, H. Weathe‘rspoon,
and E. Vollset, “Maelstrom: Transparent Error Correction for
Lambda Networks,” in Proceedings of NSDI, 2008.
[12] J. C. Mogul and K. K. Ramakrishnan, “Eliminating receive
livelock in an interrupt-driven kernel,” ACM Trans. Comput.
Syst., vol. 15, no. 3, pp. 217–252, 1997.
[13] “NAPI,” http://www.linuxfoundation.org/en/Net:NAPI.

[17] J. Padhye, V. Firoiu, D. Towsley, and J. Kurose, “Modeling
TCP throughput: a simple model and its empirical validation,”
SIGCOMM Comput. Commun. Rev., vol. 28, no. 4, pp. 303–
314, 1998.
[18] “P. Wefel, Network Engineer, “The University of Illinois
National Center For Supercomputing Applications (NCSA).
Private Communication”,” Feb 2007.
[19] “National LambdaRail,” http://www.nlr.net/, 2009.
[20] Cisco Systems, “Buffers, Queues, and Thresholds on the
Catalyst 6500 Ethernet Modules,” 2007.
[21] Intel, “IRQBALANCE,” http://www.irqbalance.org/, 2009.
[22] V. Jacobson, “Congestion avoidance and control,” SIGCOMM
Comput. Commun. Rev., vol. 25, no. 1, pp. 157–187, 1995.
[23] J. M. Smith, J. D. Chung, and C. B. S. Traw, “Interrupts,”
in Encyclopedia of Electrical and Electronics Engineering,
1999, pp. 667–673.
[24] “Ixia,” http://www.ixiacom.com/, 2009.
[25] J.-C. Bolot, “End-to-end packet delay and loss behavior in
the Internet,” in Proceedings of SIGCOMM ’93.
[26] I. Cidon, A. Khamisy, and M. Sidi, “Analysis of Packet Loss
Processes in High-Speed Networks,” IEEE Transactions on
Information Theory, vol. 39, pp. 98–108, 1991.
[27] R. L. Carter and M. E. Crovella, “Measuring bottleneck link
speed in packet-switched networks,” Perform. Eval., vol. 2728, pp. 297–318, 1996.
[28] R. Kapoor, L.-J. Chen, L. Lao, M. Gerla, and M. Y. Sanadidi,
“CapProbe: a simple and accurate capacity estimation technique,” SIGCOMM Comput. Commun. Rev., vol. 34, no. 4,
pp. 67–78, 2004.
[29] C. Dovrolis, P. Ramanathan, and D. Moore, “What Do Packet
Dispersion Techniques Measure?” in INFOCOM ’01.
[30] ——, “Packet-dispersion techniques and a capacityestimation methodology,” IEEE/ACM Trans. Netw., vol. 12,
no. 6, pp. 963–977, 2004.
[31] M. Jain and C. Dovrolis, “End-to-end available bandwidth:
measurement methodology, dynamics, and relation with TCP
throughput,” IEEE/ACM Trans. Netw., vol. 11, no. 4, pp. 537–
549, 2003.
[32] V. J. Ribeiro, R. H. Riedi, R. G. Baraniuk, J. Navratil,
and L. Cottrell, “pathChirp: Efficient Available Bandwidth
Estimation for Network Paths,” in PAM’03 Workshop.
[33] R. S. Prasad, M. Murray, C. Dovrolis, and K. Claffy, “Bandwidth Estimation: Metrics, Measurement Techniques, and
Tools,” IEEE Network, vol. 17, pp. 27–35, 2003.

[15] S. Wallace, “Lambda Networking,” Advanced Network Management Lab, Indiana University.

[34] H. Bullot, R. L. Cottrell, and R. Hughes-Jones, “Evaluation
of advanced TCP stacks on fast long-distance production
networks,” in Proceedings of the International Workshop on
Protocols for Fast Long-Distance Networks, 2004.

[16] A. Tirumala, F. Qin, J. Dugan, J. Ferguson, and K. Gibbs,
“Iperf – The TCP/UDP bandwidth measurement tool,” 2004.

[35] R. Prasad, M. Jain, and C. Dovrolis, “Effects of Interrupt Coalescence on Network Measurements,” in PAM’04 Workshop.

[14] “Teragrid,” http://teragrid.org/, 2009.

