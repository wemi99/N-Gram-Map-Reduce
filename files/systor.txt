banner above paper title

Beyond Power Proportionality:
Designing Power-Lean Cloud Storage
Lakshmi Ganesh

Hakim Weatherspoon

Ken Birman

Cornell University
lakshmi@cs.cornell.edu

Cornell University
hweather@cs.cornell.edu

Cornell University
ken@cs.cornell.edu

Abstract

Categories and Subject Descriptors C [Computer
Communication Networks]: Distributed Systems
General Terms Power Management
Keywords

1.

Cloud Storage, Power-Aware Storage

Introduction

This is an account of our exploration of low-power storage designs for the Internet Archive (IA) [2]. The IA
is a petabyte-scale (and growing) online data repository, whose aim is to archive all of the world’s (public)
data. Its collection currently comprises over 150 billion

[Copyright notice will appear here once ’preprint’ option is removed.]

short description of paper

200

Energy Consumed (MWh/month)

We present a power-lean storage system, where racks
of servers, or even entire data center shipping containers, can be powered down to save energy. We show that
racks and containers are more than the sum of their
servers, and demonstrate the feasibility of designing
a storage system that powers them up and down on
demand; further, we show that such a system would
save an order of magnitude more energy than current disk-based power-proportional storage systems.
Our simulation results using file system traces from
the Internet Archive show over 44% energy savings, a
5x improvement over disk-based power management
systems, without performance impact. We explore the
tradeoffs in choosing the right unit to power off/on, and
present an automated framework to compute the optimal power management unit for different scenarios.

150
100
50
0

NO

PO

DI

ID
NO
20
4
1
20
EA
NO 0NO 00N
0
L D DEOD NOD
DE
DE
P
C
CU
IS
EE-P
-P
U
ER
KPC
PC
CU
CU
MG
PC
U
U
U
MT

W

SK

-P

Figure 1. Energy Consumed For Different PCU
Choices
web pages, as well as a media collection that includes
millions of image and text files, and hundreds of thousands of audio and video files [2]. Brewster Kahle, the
founder of this non-profit organization, is credited with
inventing the concept of data center containers a full
decade before they were adopted and made fashionable
by market giants like Microsoft, Google, Amazon, Yahoo, etc. [11, 22]. He and his team at IA are now interested in designing the next-generation power-lean data
center container – the GreenBox [19]; our work is related to this project.
In this paper, we share a key finding from our study
- the role of the power cycle unit in storage power management. Power-proportional storage solutions power
down idle IT components to save energy; we define the
PCU as the unit chosen for powering down – e.g. disk,
server, rack, or data center shipping container. Current solutions limit themselves to disk power cycling.
Data Center (DC) containerization offers the unprecendented opportunity of treating an entire container as a
component that can be turned on and off on demand.
1

2011/2/8

Accordingly, we simulated a range of current solutions
and compared them against a model where racks, or
entire containers could be powered down. Our results
strongly indicate that using larger PCUs can result in
an order of magnitude more savings, and should be explored further. Figure 1 gives an overview of our results
for the IA; a 20-node PCU results in an 80% improvement in power savings over single disk PCU, and a 30%
improvement in power savings over single node PCU.
We posit that our findings have value beyond the
IA. Firstly, low-power cloud storage design is of central importance today. We are in the midst of a data
deluge[7] – even as you read this paper, about 100 GB
of data are being generated every second, principally to
be stored on hard disks [21]. Moreover, this number is
doubling every 18 months [28] – faster than hard disk
capacity growth (which doubles every two years [3]);
with the result that the number of disks needed to store
the world’s data is growing exponentially. An energy
footprint that is proportional to the total data stored
is, therefore, simply not sustainable. In this paper, we
study the problem of scaling IA’s storage to meet the
demands of the data deluge.
Secondly, we demonstrate the importance of looking beyond disk-power in designing low-power storage.
Disk-power-based approaches [13, 15, 17, 23–25, 29]
overlook a simple fact: 40% of the power drawn by a
data center goes towards the overheads of cooling and
power distribution [16], and is untouched by current solution designs (disks themselves, by comparison, consume only about 27% of the delivered power [29]). The
great missed opportunity of cloud storage is in not doing more to amortize this sizeable chunk of the power
cost of a data center. In this paper, we take a stab at
quantifying the benefit of amortizing this overhead. We
also take the first steps towards designing a practical
system that can spin down entire DC containers.
We make the following contributions:

• We present a simple abstraction that concisely en-

capsulates the current power-proportional storage
space, and shows its limitations.
• We present an automated framework, based on the

above abstraction, that computes the performance
and energy profile of different power management
solutions for a range of storage system parameters.
short description of paper

• We quantify the benefits of moving to larger PCUs

for the Internet Archive, and show the relevant tradeoffs.
In the next section, we give some background on the
IA. Section 3 surveys the power-proportional storage
space, and presents a simple abstraction to describe it.
We formally define PCUs in section 4, and show how to
enable larger PCUs. Section 5 presents our simulation
framework and our results. We discuss some practical
implementation issues in section 6 and conclude in
section 7.

2.

The Internet Archive

The IA was founded in 1996 with the mission of providing “universal access to all knowledge” [2]. In a
world turned digital, where information is as easy to
wipe out as it is to create, the value of such an endeavor cannot be overstated. Society’s memory is only
as large as its libraries, and it is the stated intent of the
IA to help build as complete and long-lived a memory for society as possible, and make it accessible to
everyone. What does “all knowledge” consist of? IA’s
repository currently spans billions of webpages, millions of text files, hundreds of thousands of audio and
video files, as well as a new software archive containing
over a hundred thousand program files [2]. “Universal
access” currently translates to everyone with access to
the Internet; however, the IA has been actively working
on broadening their reach beyond the Internet [20].
Before we go into further details about the IA, let
us briefly explain why it makes for a uniquely interesting case study in the area of power-aware cloud storage: Firstly, it epitomizes the problem of scaling storage to meet the demands of the data deluge; its charter, after all, is to store all data. Secondly, the IA targets long-term preservation of (and immediate access
to) data, rather than high-throughput data analysis and
allied issues; in this it differs from data intensive computing services (which have tended to dominate the
literature of late – ([9], [10], [12], etc.)). We believe
these are orthogonal problems; once there is a sustainable framework for storing data at truly vast scales,
data management/analysis services can be supported in
a staged fashion. Finally, the IA is a not-for-profit organization, and operates under constraints (limited resources - money, people, etc.) that make the problem of
scaling it more challenging; lean operation is not just
desirable, but necessary in this context.
2

2011/2/8

Let us now examine the current architecture and
operation of the IA in order to identify its chief powersaving opportunities.
Fraction of Files

Architecture

0.2
0.1

10
e+
1.
9
e+
1.
8
e+

1.

7
e+
6

1.

e+

1.

3

5
e+
4

1.

e+

1.

1

+2

e
1.

e+
1.

0
e+

File Size (bytes)

(a) Distribution of File Sizes Among Files

0.4
0.3
0.2
0.1

5

7
e+
6

1.

e+

1.

3

2

4

e+

1.

e+

1.

e+

1.

e+

1.

10
e+
1.
9
e+
1.
8
e+
1.

0
1
e+
1.

short description of paper

0.3

1.

The IA offers two distinct (free) services: Wayback
Machine, and Media Collection. The former offers
snapshots of the World-Wide Web over time (since
1996), while the latter houses IA’s collection of text,
image, audio and video files. There are some differences in how these two services function; in this study,
we shall focus on IA’s Media Collection.
The IA’s Media Collection (MC) currently spans
about 2 PB of data [8] that include public domain
books, images, audio and video files (not including
replicas). The service handles millions of requests
daily, amounting to over 40 TB [18]. The MC stores
contents as groups of files called “elements”. For example, video elements usually consist of the video file
in several popular formats, while a book element might
consist of several pages, each being represented as an
image or text file. Elements are uploaded by users as
well as IA staff, and are written to two dedicated “import nodes”; when these nodes fill up, two new nodes
are drafted for the purpose. A monitoring service ensures that this two-way replication is maintained in the
face of failures. Additional replicas are created based
on item popularity – highly popular elements are manually replicated and distributed to other nodes for load
balancing.
The IA employs about six front-end web nodes to
handle user requests, and over 2500 back-end storage nodes to host content in their primary data center. The web nodes maintain a content index (a replicated MySQL database), where elements are indexed
by their name and metadata (if any). User search terms
are matched against this index to retrieve relevant element names. However, the content index maintains no
information about the element location; to retrieve location information, web nodes broadcast a UDP message containing the relevant element names to all the
storage nodes. Storage nodes maintain a list in memory
of the names of all elements they store; and if they find
a match among the requested elements, they respond
to the broadcast message. The web node then redirects
the user to the storage node, which serves the requested
content. Storage nodes are typically commodity servers
with a low-power CPU and four disks.

Fraction of Accesses

2.1

0.4

File Size (bytes)

(b) Distribution of File Sizes Among Accesses

Figure 2. IA Workload Characteristics
It is worthwhile to pause here to partially explain
what may appear surprising choices in the above description (manual load balancing, lack of location indices, item location by UDP-broadcasts, etc.). One of
the IA’s guiding principles is simplicity in design [18].
Indeed, this is a necessity given their lean operations,
with a very small staff count, high staff turnover, and
limited resources. Complex systems potentially mean a
higher initial outlay, greater risk of bugs, longer training time for new staff; all of which are luxuries the
IA can ill-afford. Finally, the reality is that this extremely basic design has worked satisfactorily for over
a decade, and has gained the IA a wide user base.
2.2

Workload

Figures 2 and 3 present some details of the IA workload. It is an almost read-only workload as data uploads
3

2011/2/8

Data Type
Data Duration
Data Source
Fraction of reads (GETs)
Access rate

HTTP logs
1 year
886-node IA MC data center
≈ 99%
≈ 30m accesses/day

Figure 3. IA Workload Details
typically occur over a separate interface. Some important observations are:
1. There is a lot of data that never gets accessed (dark
data)
2. Idle periods for nodes often last hours
3. Both primary and secondary (mirror) nodes respond
to file requests
4. Average CPU utilization is very low
Many of these observations translate to powersaving opportunities.
2.3

Power-Saving Opportunities

• Live data is a small fraction of total data: The

amount of data that is accessed at any time is a small
fraction of the total data stored; further, a lot of data
is never accessed for the interval we studied. This
indicates that much of the data need not be kept live
for much of the time.
• All data is duplicated: It is not necessary that all

replicas be kept live at all times. We shall show
how smart replica placement can create significant
energy saving opportunities.
• CPU utilization is low: For every four disks con-

suming 10 W (each), there is one CPU consuming
200 W. Given the low average CPU utilization, one
way to save power would be to increase the disk-toCPU ratio.
• Greenbox ideas: For some other ideas to save

power (that are orthogonal to the ones we discuss
here), see [19].

3.

Related Work: Power-Proportional
Storage

The principle behind power-proportional storage is that
power should track utilization; live data is usually a
very small fraction of total data in any large-scale storage system, and it follows that considerable power can
short description of paper

be saved if the disks housing non-live data can be powered down.
The concept of power-proportional storage is less
than a decade old, but the literature is already crowded
with creative solutions. We present a brief survey here,
and in doing so attempt to distil the principles that
govern this space of solutions. A close examination
of power-proportional storage solutions leads to the
observation that they can be uniquely specified by two
basic parameters:

1. Data Localization Target: Power-proportional storage schemes attempt to localize data accesses to a
subset of the system so that the rest can be powered down. The data localization target parameter
encodes this concept. For instance, MAID [13] concentrates popular data on a new set of “cache” disks,
while PDC (Popular Data Concentration) [24] uses
a subset of the original disk set to house the popular data. Power-aware caches [14] attempt to house
the working set of spun-down disks in the cache,
to increase their idle time. Write-offloading [23] is
a technique that can layer on top of each of these
solutions to temporarily divert write-accesses from
spun-down disks to spun-up ones, and so is a scheme
to localize write accesses. SRCMap [25] is similar to MAID and PDC (and additionally uses writeoffloading), but is a more principled version of both.
KyotoFS [15] is similar to write-offloading, but uses
the log-structured file system to achieve write diversions.
2. Architecture: Power-proportional storage systems
often add levels to the storage hierarchy in order to
create disk power-down opportunities. The architecture parameter encodes the storage hierarchy of a
given solution. For instance, the standard storage hierarchy puts primary memory (RAM) ahead of spinning disks. Power-proportional storage solutions
add spun-down disks to the tail of this hierarchy.
MAID uses an additional set of disks (cache-disks)
between memory and the original disk set. PDC,
power-aware caching, SRCMap, write-offloading,
and KyotoFS all use the original disk set, and add no
new levels. Hibernator [29] uses multi-speed disks,
as does DRPM [17]. HP AutoRAID [27] divides
the disk-set into a smaller, high-performance, highstorage-overhead RAID 1 level, and a larger, low4

2011/2/8

Energy Consumed Over 2HR IA Trace

Energy Consumed (kWh)

100

Energy Consumed
% Energy Saved

600

80

500
400

60

300

40

200

20

100
0

NO

MA
ID

PO
W

ER

MG

MT

-1

MA

0

ID

00

GB

Energy Saved (%)

700

-ID

EA

L

Figure 5. Limited Energy Savings From Disk Power
Management

case resulted in a saving of 8.2%. The takeaway here
is that disk power management solutions are limited in
their benefit, with an upper limit (for IA) of less than
10% energy savings. Given the complexity of several
of these solutions, the argument for their adoption is
weak.
3.2

As we saw above, the current power-proportional storage space has inherently limited benefit. The reason
is that power-proportionality alone is not enough; a
power-proportional storage solution could still waste
significant amounts of energy in the following ways:
• As much as 40% of the power consumed by the

storage system goes towards power distribution and
cooling overheads [16]. While power-proportional
storage solutions might help reduce cooling needs,
they leave much of this overhead untouched. (Compare with disk power, which accounts for only about
27% of total storage power [29].)

performance, low-cost RAID 5 level. PARAID [26]
is a power-aware variant of AutoRAID.
Figure 4 describes a representative set of current solutions as instantiations of this basic abstraction. Under
the “data localization target” heading, figure 4 lists the
invariant that each solution attempts to maintain over
time. The following notation is used:

D = {d1 , d2 , . . . , dn } = the set of disks
A = {dn+1 , dn+2 , . . . , dn+m }
= additional set of disks, m ≥ 0
Ci = {the contents of disk di (excl. replicas)}
Wi = {the working set of disk di } ⊆ Ci
Ri = {the set of replicas on disk di }
Si = {set of possible states of disk di }
si = {state of disk di }

3.1

Limitation of Disk Power Management
Solutions

We built a power-aware storage system simulator,
based on the above abstraction. Section 5.1 describes
the simulator in detail, but we present a relevant result
here in figure 5. For a 2-hour file access trace from
IA, we configured the simulator to mirror one of the
MC data centers (see table 9 for a listing of the parameters). We then simulated a MAID system with a
100GB MAID disk; the result was a 4.8% saving in energy. Further, we compared this with an idealized case
where all of the back-end disks are powered off for
the entire run; this case represents an ideal for any of
the above disk power management solutions. The ideal
short description of paper

Beyond Power-Proportionality

• Storage systems typically replicate data for failure-

resilience and/or performance. Mindful replica placement could allow some or all replicas to be turned
off during periods of light load.
• Additional consumers of power, that are neglected

by current solutions, include:
1. The data center networking infrastructure
2. Non-disk components of servers, such as CPU,
memory, fan, etc.
3. Non-IT DC components, such as lights, fail-over
power generators, etc.
In essence, an extensive infrastructure exists to support the storage system – providing services such as
power distribution, cooling, failure-resilience (redundancy), etc. – and any power-saving solution that neglects to take this into account is necessarily incomplete. The next section discusses how to go from
power-proportional to power-lean.

4.

Power Cycle Unit

We define the power cycle unit as the resource unit
that the power management scheme operates over.
This is the unit whose power state is manipulated
to track utilization. For example, disk power management schemes manipulate the disk power state
(ON/OFF/possibly low-power states corresponding to
5

2011/2/8

Solution
MAID
PDC
PA Cache
SRCMap
Hibernator

Data Localization Target
Architecture
P
P
m > 0, Si = {0, 1}
di ∈A Ci ⊇
di ∈D Wi
Cache disks (A) used to hold working set of orig. disk set D. Disks can be on/off
P
P
m = 0, Si = {0, 1}
di ∈D∧si =1 Ci ⊇
di ∈D Wi
Powered-up disks in D contain working set of D. Disks can be on/off
P
P
Ccache ∪ di ∈D∧si =1 Ci ⊇ di ∈D Wi
m = 0, Si = {0, 1}
Cache and powered-up disks in D together contain working set of D. Disks can be on/off
P
P
P
m = 0, Si = {0, 1}
di ∈D∧si =1 Ri ∪
di ∈D∧si =1 Ci ⊇
di ∈D Wi
Powered-up disks in D (orig. contents + replicas) contain working set of D. Disks can be on/off
P
P
m = 0, |Si | > 2
di ∈D∧si >0 Ci ⊇
di ∈D Wi
Powered-up disks in D contain working set of D. Disks are multispeed
Figure 4. The Current Power-Aware Storage Solution Space

lower speeds); CPU power management schemes manipulate CPU power (typically through frequency tuning). Our contention in this paper is that other PCU options, which have not been explored thus far, promise
significantly bigger energy savings.

ing power distribution, networking, and cooling equipment; powering these down offers energy savings far
beyond the limited disk power management space.

4.1

While larger PCUs are now physically possible, work is
required to make them practical. Powering down a rack
is not practical if it would result in service interruption
or network disruption. However, as we suggest above,
commodity racks exist that can be introduced into, or
taken out of, the data center network without interrupting or disturbing service. These have their own network switch, power distribution unit (often softwarecontrolled), and cooling equipment, and thus provide
fault-isolation from the rest of the network. There is another issue, however – without some work, rack powerdown opportunities (that is, all of the servers in the rack
being simultaneously idle) are likely to be few. We shall
now show how we could create power-down opportunities for different PCUs in the IA context, through appropriate data organization.
PCU-aware data organization essentially consists of
two steps:

Key Opportunity: Modularity

The online services hosting space is evolving so rapidly
that data center design standards are a moving target. However, they are characterized by one guiding
principle – modularity. Agility, and rapid scalability
are imperatives for successful online services – and
both require modularity in design. Consider the facts:
rapid expansion needs ushered in the concept of “commodity servers” – preassembled servers conforming
to the most popular configurations prevalent in industry, ready for purchase off the shelf, deployable simply by plugging them into the data center. The concept
has now expanded to racks, which are increasingly becoming the unit of choice for expansion. “Commodity
racks” have servers, top-of-rack switches ([6]), power
distribution units ([4]), and even in-rack cooling equipment ([1]) pre-installed. Purchasing and commissioning a rack is now a mere matter of hours – the “rackand-roll” phenomenon [5]. Further along this path, entire data centers have now been commoditized – the
data center shipping container – an idea that originated
with the IA’s founder - Brewster Kahle.
This modularity at multiple levels translates to a
new opportunity for power management solutions: we
now have the ability to power down racks, or even entire containers. Each of these potential PCUs houses
not only servers and disks, but also their correspondshort description of paper

4.2

Enabling Different PCUs: PCU-Aware Data
Organization

1. Each data item must be spread (striped/mirrored)
across PCUs, rather than within them. Thus, assuming some degree of data redundancy, one or
more host PCUs may be down without impacting
the availability of that item.
2. Data access must be localized (as far as possible)
to a subset of the PCUs so that others are idle and
may be powered down. This is achieved by directing
6

2011/2/8

70

pcu
node
disk

% PCU/Node/Disk down

60
50
40

MAID

30

Node
Disk
Rack
Data Item Replica
Powered-Down State

Back-end

20
10
0

DI

SK

NO
DE
Data Organization Unit

40
NO

DE
-

RA
CK

Figure 6. Impact of Data Organization Scheme on
PCU Power-Down Opportunities

(a) PCU=Rack

accesses to an item to the more active among its host
PCUs.
Figures 7(a), and 7(b) illustrate PCU = Rack, and
PCU = Node, respectively. Note how replica placement
changes with PCU; note, also, the creation of idle PCUs
through selective access of more active replica hosts.
Figure 6 shows us the importance of PCU-aware data
organization. Having set the PCU to 40-node racks, we
varied the data organization unit (the unit across which
replicas are distributed). As expected, we see that unless replicas are distributed across the given PCU (40node racks, in this case), there are no opportunities for
powering them down. When the replicas are distributed
across disks, or nodes, we see plenty of disk and node
power-down opportunities, but no rack power-down
opportunity. Thus, PCU-aware data organization (and
retrieval) is key to enabling larger PCUs.

5.

Evaluation

The aim of this study is to quantify the potential energy
savings from using larger PCUs, for IA and beyond. We
wish to answer the following questions:
1. Internet Archive: What choice of PCU maximizes
energy savings for the Internet Archive without hurting performance?
2. Beyond the IA: How is this choice affected by system parameters such as data organization, request
rate, and cache size?
3. Simulator Sensitivity: How far do simulator parameters affect the results?
short description of paper

MAID

Node
Disk
Rack
Data Item Replica
Powered-Down State

Back-end

(b) PCU=Node

Figure 7. System Model
We describe our methodology, and then present our
findings.
5.1

Methodology

We use simulations to explore the PCU space, for two
reasons: Firstly, for a problem of this scale, a real deployment study is impractical. Secondly, we wish to explore a number of different PCU options, and the large
combinatorial space of solutions and their configuration parameters does not allow for a practical deployment study.
5.1.1

Simulator

Our simulator models the power-proportional storage
abstraction described in section 3, and allows different solutions to be simulated by specifying their architecture and data localization target. The model we
7

2011/2/8

Parameter
Data Layout

Description
Redundany scheme employed

Disk Power (W)
(Up/Down/Tran)
Node Power (W)
(Up/Down/Tran)
Rack Power Overhead (%)
(Up/Down/Tran)
Disk Access Time (ms)
Disk Transition Time (s)
Node Transition Time (s)

Power consumed by disk when up, down, or transitioning
between up and down
Power consumed by node (over and above that consumed by its
disks) when up, down, or transitioning between up and down
Power consumed by rack (over and above that consumed by its
nodes) when up, down, or transitioning between up and down
Time taken to retrieve data from disk that is up
Time taken by disk to go between up and down states
Time taken by node (over and above that taken by its disks)
to go between up and down states
Time taken by rack (over and above that taken by its
component nodes) to go between up and down states
The intervals at which all PCUs are examined
and idle ones powered down
The interval after start of simulation when
power checking begins
An exponentially weighted disk access count threshold
below which the disk is considered idle
(optional) Force this target number of disks to be
powered down during power checks, whether idle or not
MAID disk capacity
Actual number from an IA MC data center
Actual number from an IA MC data center

Rack Transition Time (s)
(20/40/100/200)-node rack
Power Check Interval (hr)
Power Management Start Time (hr)
Disk Power Down Threshold
Target Disk Down Count
Cache Size
Number Of Nodes
Number Of Disks/Node

Value
PCU-aware,
2-way mirroring
10/2/10
200/5/200
50/0/50
8
6
30
300/300/420/600
0.5
0.5
10
50%
100 GB
886
4

Figure 9. Simulator Parameters (applicable unless specified otherwise)

Trace

Config

Logs

PCUs Nodes Disks

Replica
Store

State

Event
Loop

StorageSystem

PowerManager

Figure 8. Simulator Architecture

the system, recording latency, power consumption, etc.
Figure 7 shows the system model with PCU = Rack,
and PCU = Node respectively.
The simulator is written in Python, and comprises
less than 2000 lines of code. It is event-based, and takes
as input a trace file of data accessses, as well as a configuration file that specifies the solution architecture,
and the capacity, power and latency specifications of its
components. It then models an execution of the specified solution on the input trace, and returns an execution log that details the power and performance profile
of this run. Figure 8 shows the simulator architecture,
while figure 9 presents the standard simulation parameters.
5.1.2

work with for our PCU explorations is a MAID-style
system, with PCU-aware back-end data organization.
Given the system specifications (node and disk capacity, bandwidth, power ratings, PCU membership information, PCU power overhead, and transition time, etc),
we simulate the progress of each file request through
short description of paper

Data

For our experiments, we use the Internet Archive’s MC
access logs for the week of April 3-9, 2009. These
reads ) of very close
access logs have a read-ratio ( # #accesses
to 1 (0.9926); we have, therefore, limited ourselves
to reads in our experiments. Supporting writes is the
subject of future work (see section 6 for more on this).
8

2011/2/8

Trace 1
6 hrs
6.5m
1.7
7.73
7797.77
110322
833

Trace 2
6 hrs
7m
1.3
20.74
8338.12
184424
838

Trace 3
6 hrs
6.6m
1.5
7.73
7862.95
120983
835

7000
6500
Energy (kWh)

Attribute
Duration
# accesses
Avg. access size (MB)
Max access size (GB)
Avg # accesses to a node
Max # accesses to a node
# Nodes accessed

6000
5500
5000
4500
4000

D

O

N
U

U

PC

E-

PC

< 100 in 10^5 accesses
< 10 in 10^5 accesses
< 1 in 10^5 accesses

Latency (ms)

10^5
10^4
10^3
10^2
10^1
10^0

0N

20

0N

10

PC

E-

D

O
PC

E-

D

U

U

U

U

PC

PC

E-

D

O

O

N
E-

D

O

U

U

PC
E-

D

N

40

20

O

N

PC
K-

IS

short description of paper

E-

10^6

% Disks Down % Nodes Down % PCUs Down

(b)

Internet Archive

Question: What is the optimal PCU size for the IA?
For the parameter set listed in table 9, which is intended to approximate the IA store, we ran a 24-hour
trace (from April 3, 2009). Our findings are shown in
figure 11. Figure 11(a) shows that as we increase PCU
size, a sweet-spot (minimum) is achieved for energy at
the two configurations PCU = 20-node rack, and PCU
= 40-node rack. At these configurations, we obtain energy savings over disk power management solutions of
over 44%, and over node power management solutions
of 30%. In energy, this translates to over 3MWh saved
per day. Further, figure 11(b) shows that the 20-, and
40-node PCU configurations actually perform somewhat better than the node PCU configuration! Each set

U

(a)

D

5.1.3

D

PC

U

Unless otherwise specified, each data point presented in
the following section is the averaged result of running
6-hour traces from three different days of this week (a
Monday, Tuesday, and Friday, the same set of hours
being picked from each day). Figure 10 gives details of
these traces.
The traces are basically HTTP GET logs, and specify, for each file access, the access time, the file details (name, size), as well as the storage node details
(id, disk number). These accesses are essentially cachemisses from the front-end web nodes. Recall that file
location (for a cache-miss) is obtained by UDP broadcast to all the storage nodes. These accesses, thus, provide the storage node data as well. However, we manipulate this information slightly to conform to different data organization layouts. Given a data organization
scheme – PCU-aware, 2-way mirroring, for example –
we statically map each disk to a “mirror disk” such that
the mirror disk is on a different PCU from the original
disk. An access request to any item on either disk is
then directed to the more active of the two. Support for
dynamic, per-file mapping is planned in future work.

0
20

E-

U

PC

E-

D

D

O

O

N

0
10

N
40

O

N
20

PC

U

PC

E-

K-

D

O

N

IS

D

Figure 10. Trace Characteristics

60
50
40
30
20
10
0
60
50
40
30
20
10
0
70
60
50
40
30
20
10
0

100NODE-PCU
200NODE-PCU
20NODE-PCU
0

5

10
15
Simulation Time (hr)

40NODE-PCU
DISK-PCU
NODE-PCU
20

(c)

Figure 11. Computing Optimal PCU Size for the Internet Archive
of three bars in this graph shows the the highest latency
seen in the 99.9-, 99.99-, and 99.999- th percentile
of accesses respectively (left-to-right). We see that all
configurations have acceptable performance, with over
99.9% accesses seeing no delay.
Figure 11(c) explains this latency distribution. For
each configuration, it tracks the number of PCUs,
nodes, and disks that are powered down over the length
of the simulation. We see that for all of the configu9

2011/2/8

2300

Energy (kWh)

2200
2100
2000
1900
1800
1700

D

D

O

N

0
20
U

PC

E-

U

U

U

PC

PC

E-

E-

PC

E-

D

D

O

O

N

0
10

N
40

O

N
20
U

U

PC

PC

E-

K-

D

O

N

IS

D

(a) 5% Rack Overhead
2300
2200
2100
2000
1900
1800
1700
1600

0N

20

0N

10

PC

E-

D

O
PC

U

U

U

U

PC

E-

E-

D

D

O

O

N

U

PC

E-

D

O

PC
E-

D

N

40

20

O

N
U

PC
K-

IS

D

(b) 25% Rack Overhead
2300
2200
2100
2000
1900
1800
1700
1600
1500
1400
1300

U

U

PC
E-

PC
E-

D
O

0N

20

U

PC

U

PC

E-

D
O

0N

10

D
O

N
E-

D
O

U

U

PC

PC

E-

D

N

40

20

O

N

K-

IS

D

(c) 100% Rack Overhead

Figure 12. Effect of Rack Power Overhead on Optimal
PCU Size

rations with PCU > node, the number of PCUs down
stays constant after the initial power check interval.
This means that no access goes to a powered-down
PCU, with the result that PCU power downs have no
performance penalty!
Note: the remaining results all use three 6-hour
traces to generate each data point.

short description of paper

Question: How does this choice of optimal PCU depend on Rack Power Overheads? Clearly, the higher
the power overhead of a rack, the more energy savings obtained by powering it down. For our results
above, we used a rack power overhead of 50%; so,
for example, a 40-node rack would have a power over50
head of 100
∗ (40 ∗ 200) = 4000W . We chose this
as a reasonably conservative value, given the industry
rule-of-thumb that 1W of cooling is needed for every
Watt going to servers (ie.. a 100% overhead). However,
we would like to compute the minimum rack power
overhead, at which it becomes worthwhile to consider
PCUs that are greater than node. Figure 12 shows that
this minimum overhead value is close to 25%. At overhead values of 5% or less, we actually waste energy
if we power down racks. However, at overhead values
of 25% and over, a 20-, or 40-node rack is the optimal
PCU for the IA, with energy savings increasing with
overhead.
Question: How does this choice of optimal PCU
depend on Rack Transition Time? As rack transition
time increases, we expect that the energy savings from
powering the rack down, decreases. Therefore, we can
expect a maximum rack transition time beyond which
powering down racks does not make sense. However,
as we see in figure 13, this limit is not reached for the
transition time values we explored. Even at a conservative estimate that it takes 10 minutes to power up
a 40-node rack – this is in addition to the power-up
time of its component nodes – we see that the 40-node
rack continues to be an optimal PCU choice for the IA
(figure 13(b),13(d)). We also see that halving, or doubling the transition time does not affect performance
significantly – which is in agreement with our earlier
observation that no accesses hit powered-down racks.
Takeaway: Expanding the PCU from node to rack results in a 655kWh energy savings in just one day, in
a very small (886-node) container. Extrapolating from
these results, we could expect to save about 20MWh
per month, per container, just using this simple technique. In what follows, we show how these savings can
be further increased, and generalized beyond the IA
context.

10

2011/2/8

Energy (kWh)

Transition Time Halved

Transition Time Doubled

1800

1800

1700

1700

1600

1600

1500

1500

1400

1400

1300

1300

1200

1200

1100

1100

1000

1000

Latency (ms)

U
C

P
E-

U
C

U

U

C

P
E-

D

D

O

N

0
20

P
E-

C

U

P
E-

D

O

O

N

0
10

N
40

D

PC

U
C

(b)
10^6

< 100 in 10^5 accesses
< 10 in 10^5 accesses
< 1 in 10^5 accesses

10^5

O

ED

P
K-

U
C

P
E-

U

U
C

C

P
E-

D

O

N

D

P
E-

U

C

10^6

N
20

O
N

IS
D

0
20

D

O

O

N

0
10

N
40
P
E-

U

U
C

PC

D

O

ED

P
K-

N
20

O
N

IS
D

(a)

< 100 in 10^5 accesses
< 10 in 10^5 accesses
< 1 in 10^5 accesses

10^5

10^4

10^4

10^3

10^3

10^2

10^2

10^1

10^1

10^0

10^0

0N

20

0N

10

D
O

D
O

PC

EU

U

U

U

U

U

PC

E-

PC

E-

D

O

N
PC

E-

D

O

PC

U

U

PC

E-

D

N

40

20

O

N

E-

PC

D
O

U

U

PC

E-

PC

PC

E-

D
O

K-

0N

IS

D

20

0N

10

D

O

N
E-

D

O

U

U

PC

E-

D

N

40

20

O

N
PC

K-

IS

D

(c)

(d)

Figure 13. Effect of Rack Transition Time on Optimal PCU Size
5.1.4

Beyond IA

Question: How does Optimal PCU Choice depend on
Data Organization Scheme? We now look beyond IA’s
two-way mirroring, and see how PCU choice is affected by different data organization schemes. Figure
14 shows the result of running the same trace over a
succession of data striping schemes – (n, m), where n
is the total number of chunks in a stripe, and m is the
least number of chunks needed to reconstruct the data
item. Each configuration is represented as nm PCUsize, where PCU-size can either be node, or 40-node
rack. We see that energy savings increase as overhead
(n/m) increases (the higher the overhead of the striping
scheme, the more redundant fragments there are whose
host PCUs can be powered down), and decrease as fragmentation rate (n) increases (the higher the fragmentation rate, the bigger the set of PCUs each data item
is spread over; thus increasing inter-PCU dependencies, and reducing PCU power-down opportunities).
As a concrete example, we see that energy savings increase as we increase overhead from (6,4) to (6,3). On

short description of paper

the other hand, energy savings decrease as we increase
fragmentation from (2,1) to (6,3) to (8,4). We also see
that, for all striping schemes, setting the PCU to node
leads to having higher node and disk down-counts; consequently, node power cycling has more latency spikes
than rack power cycling.
Question: How does Optimal PCU Choice depend on Cache Size? Another parameter of interest
is cache size (note that ‘cache’ here refers to MAID
disks). Large storage designs increasingly use significant amounts of non-volatile memory as a read/write
cache. Terabytes of NV-RAM, or solid state storage are
now well within the realms of possibility. We wished
to see how much cache size impacted power-down opportunities. Figure 15 shows the surprising result that
cache size has little impact on energy savings. The
explanation is that our traces consist of accesses that
missed front-end caches; this workload, therefore, is
inherently resistant to caching.

11

2011/2/8

1500

10^5

1400

10^4

1300
1200

10^2
10^1

1000

10^0

2
4
4
6
6
6
6
8
8
NO 1_RA 3_NO 3_RA 3_NO 3_RA 4_NO 4_RA 4_NO 4_RA
DE CK DE CK DE CK DE CK DE CK

pcu actual
node actual
disk actual

60

10^3

1100

21_

< 100 in 10^5 accesses
< 10 in 10^5 accesses
< 1 in 10^5 accesses
% PCU/Node/Disk down

10^6

Latency (ms)

Energy (kWh)

1600

50
40
30
20
10

21_ 21_ 43_ 43_ 63_ 63_ 64_ 64_ 84_ 84_
NO
R
N
R
N
R
N
R
N
R
DE ACK ODE ACK ODE ACK ODE ACK ODE ACK

(a)

0

21_ 21_ 43_ 43_ 63_ 63_ 64_ 64_ 84_ 84_
NO
R
N
R
N
R
N
R
N
R
DE ACK ODE ACK ODE ACK ODE ACK ODE ACK

(b)

(c)

Figure 14. Impact of Data Organization Scheme on Optimal PCU Size
1220

10^6

1200

< 100 in 10^5 accesses
< 10 in 10^5 accesses
< 1 in 10^5 accesses

Latency (ms)

Energy (kWh)

1180
1160
1140
1120

10^4
10^3
10^2

1100
10^1

1080
1060

pcu actual
node actual
disk actual

60
% PCU/Node/Disk down

10^5

50
40
30
20
10

10^0

0

(c)

Figure 15. Impact of Cache Size on Optimal PCU Size
Question: How does Optimal PCU Size depend on
Access Rate? Finally, we partially address the question of how optimal PCU size depends on file access rates by looking at the results from two different
traces, one having 1.4 times the original access rate
(figure 16(a)), the other 0.4 times the original access
rate (figure 16(b)). While this doesn’t comprise a wide
range of access rates, it does show that energy savings
increase when access rate is lower, but the choice of
optimal PCU size does not change over the access rates
we explored.
5.1.5

Simulator Sensitivity

It is important to ensure that our results are not artifacts
of the simulator settings. We now show that our results
are fairly robust to simulator fine-tuning.
Figure 17 shows that our findings are largely independent of changes in the power-check interval, target disk-down count, and disk-down threshold. In figure 17(b), we see that energy savings are significantly
reduced by over-aggressive disk power down (forcing
100% of the disks to be down at every power-check
interval). However, PCU-size ordering with respect to
short description of paper

total energy consumption is not affected by these simulator tuning parameters. Thus, whatever the value of the
power-check interval, target disk-down count, or diskdown threshold, the choice of optimal PCU stays the
same.

6.

Discussion

We have examined PCU choices for a range of different
storage system settings; our findings strongly suggest
that disk power management is a dead end, and larger
PCUs are a very promising direction to follow. However, there are some issues we did not address during
our discourse; we examine some of them here.
• What About Writes? IA’s read-mostly workload al-

lowed us to concentrate on reads to the exclusion
of writes. While we plan, in future work, to revisit our design to add support for writes, we believe that our findings are still widely applicable.
The reason is that any truly global-scale service will
need to design separately for the hot data (which includes writes) and the less-hot data. With very large
volumes of data, storage for hot data (necessarily
12

2011/2/8

K

E

D

K

AC

R

B_

O

E

D

K

E

AC

N

B_

G

00
10

G

00
10

R
B_

0G
10
O

D

K

AC

N
B_

R

B_

O

AC

N

B_

E

D

K

E

D

AC

R

K

E

(b)

0G
10

G
10

G
10

O

R

N

B_

B_

O

AC

K

E

D

O

N

B_

G

B_

1G

1G

00
10

G

00
10

R
B_

0G
10

N
B_

AC

K

D

O

R

B_

N

B_

0G
10

G
10

G
10

E

D

K

E

D

K

E

AC

R

AC

O

R

N

B_

B_

1G

1G

B_

O

D

AC

O

N

B_

G

G

00
10

00
10

R

N

B_

B_

0G
10

E

K

D

E

K

AC

O

R

N

B_

B_

0G
10

G
10

G
10

D

AC

O

R

N

B_

B_

1G

1G

(a)

1800

1800

1700

1700
1600

1500

Energy (kWh)

Energy (kWh)

1600

1400
1300

1500
1400
1300
1200

1200

1100

1100

1000

1000

900

U

PC

U

PC

ED

O
0N

20

U
PC

E-

ED

O
0N

10

D
O

N

40
U
PC

E-

U

U

PC

U

U

PC

D
O

N

20

ED

O

N
PC

K-

ED

PC

U
PC

E-

ED

O
0N

IS

D

20

O
0N

10

U

U
PC

E-

D
O

D
O

N

40

N

20
PC

U

PC

ED

O

N

K-

IS

D

(a) 1.4X Access Rate

(b) 0.4X Access Rate

Figure 16. Effect of Access Rate on Optimal PCU Size
1240

1220

1240

1220

1200

1220

Energy (kWh)

1200

1200

1180

1180

1180

1160

1160

1160

1140

1140

1140

1120

1120

1120

1100

1100

1100

1080

1080

1080

1060

1060

1060

K

E

D

K

AC

R

0_
10

O

N

0_
10

E

D

AC

O

_R
50

_N
50

E

K

D

AC

O

_R
20

E

K

D

AC

O

_N
20

R
2_

N
2_

K
AC
R
E
0_
10 OD
N
0_
10 CK
A
_R
80 DE
O
_N
80 CK
A
_R
60 DE
O
_N
60 CK
A
_R
40 DE
O
_N
40 CK
A
_R
20 DE
O
_N
20 K
AC
R
0_ DE
O
N
0_

K

E

D

E

K

AC

D

O

AC

R

r_
1h

O

N

r_
1h

_R
in

m
15

E

K

AC

D

O

_N
in

m
15

_R

_N

in
5m

in
5m

(a) Power Check Interval

(b) Target Disk-Down Count

(c) Disk Power-Down Threshold

Figure 17. Result Sensitivity to Simulator Settings
a small fraction of the total data) can be designed
for performance, while the much larger back-end
store will need to be designed for volume. Our PCU
schemes are targetted at this back-end store.

ues to grow, it is necessary to separate the problem
of storage from computation; the former must emphasize scalability and hence power-awareness. The
latter can be designed on top of the storage solution
in a staged fashion.

• What About Bandwidth? One issue with PCU-aware

data organization is that it goes against the design
principle of putting nodes that communicate a lot
in the same rack, or under the same switch. When
PCU = Rack, we spread data across PCUs, thus
potentially impacting data retrieval latency. We are
working on factoring this cost into our model.
• Powering Down versus Over-Subscription: An oft-

made argument against power-aware storage solutions is that it is economically better to put idle
equipment to use rather than to power it down. This
argument breaks down in a large-scale data storage
scenario. PB-scale data stores, even at the outer limit
of their bandwidth capabilities, cannot serve all of
the data they host simultaneously. As data continshort description of paper

7.

Conclusion

Information is the currency of our times, and as the volume of digital data continues to grow exponentially,
designing power-lean, sustainable storage systems assumes central importance. We show that the current
power-proportional storage space has limited potential,
and that in order to scale with the data, we need to go
beyond power-proportionality towards power-lean systems that address the overheads of cooling, power distribution, and networking. We show how to design systems that can power cycle over racks, or even entire
data center containers, with an order of magnitude improvement in energy savings.
13

2011/2/8

Acknowledgments
We would like to thank Brewster Kahle and the IA
folk for sharing their data, and for their whole-hearted
support and enthusiasm. We hope this work will be
useful to them. We would also like to thank Deniz
Altinbuken for her help in gathering statistics about the
IA data. Finally, we are grateful to AFRL, NSF and
Microsoft for their support of this effort.

References
[1] High density in-rack cooling solutions for server
racks, computer rooms, server rooms & data centers.
URL http://www.42u.com/cooling/
in-rack-cooling/in-rack-cooling.htm.
[2] The internet archive. http://www.archive.org.
[3] Moore’s law. Wikipedia.
[4] Switched rack pdu. URL http://www.apc.com/
products/family/index.cfm?id=70.
[5] Cisco data center infrastructure 2.5 design guide, 2007.
[6] Data center top-of-rack architecture design, 2009.
[7] Data, data everywhere. Special Report, The Economist,
February 25 2010.
[8] In Personal Communication with Brewster Kahle and
the Internet Archive Staff, January 14 2010.
[9] H. Amur, J. Cipar, V. Gupta, G. Ganger, M. Kozuch,
and K. Schwan. Robust and flexible power-proportional
storage. In Proceedings of Symposium on Cloud Computing (SOCC), 2010.
[10] D. Andersen, J. Franklin, M. Kaminsky, A. Phanishayee, L. Tan, and V. Vasudevan. Fawn: A fast array of wimpy nodes. In Proceedings of Symposium on
Operating Systems Principles (SOSP), 2009.
[11] B. Baumgart and M. Laue. Petabyte box for internet
archive, November 2003.
[12] A. Caulfield, L. Grupp, and S. Swanson. Gordon: Using
flash memory to build fast, power-efficient clusters for
data-intensive applications. In Proceedings of the 14th
International Conference onf Architectural Support for
Programming Languages and Operating Systems (ASPLOS), 2009.
[13] D. Colarelli, D. Grunwald, and M. Neufeld. The case
for massive arrays of idle disks (maid). In Proceedings
of the conference on File and Storage Technologies
(FAST), 2002.
[14] Q. Z. Francis, F. M. David, C. F. Devaraj, Z. Li,
Y. Zhou, and P. Cao. Reducing energy consumption of
disk storage using power-aware cache management. In
Proceedings of the International Symposium on HighPerformance Computer Architecture (HPCA), Febuary,
2004.
short description of paper

[15] L. Ganesh, H. Weatherspoon, M. Balakrishnan, and
K. Birman. Optimizing power consumption in large
scale storage systems. In HotOS, 2007.
[16] A. G. Greenberg, J. R. Hamilton, D. A. Maltz, and
P. Patel. The cost of a cloud: research problems in data
center networks. Computer Communication Review,
2009.
[17] S. Gurumurthi, A. Sivasubramaniam, M. Kandemir,
and H. Franke. Drpm: Dynamic speed control for
power management in server class disks. Computer Architecture, International Symposium on, 2003.
[18] E. Jaffe and S. Kirkpatrick. Architecture of the internet
archive. In Proceedings of The Israeli Experimental
Systems Conference (SYSTOR), 2009.
[19] B. Kahle.
Project greenbox.
http:
//backyardfamilyfarm.wikispaces.com/
Project+Greenbox.
[20] R. Kaushik. Spreading the digital word. ExtremeTech,
April 29 2003. URL http://www.extremetech.
com/article2/0,3973,1047454,00.asp.
[21] P. Lyman, H. Varian, P. Charles, N. Good, L. Jordan,
and J. Pal. How much information? executive summary.
School of Information Management and Systems, UCBerkeley, 2003.
[22] C. Metz. Sun packs 150 billion web pages into meat
locker. March 2009.
[23] D. Narayanan and A. Donnelly. Write off-loading:
Practical power management for enterprise storage,
2008.
[24] E. Pinheiro and R. Bianchini. Energy conservation
techniques for disk array-based servers, 2004.
[25] A. Verma, R. Koller, L. Useche, and R. Rangaswami.
Energy proportional storage using dynamic consolidation. In Proceedings of the File and Storage Systems,
2010.
[26] C. Weddle, M. Oldham, J. Qian, A.-I. A. Wang, P. Reiher, and G. Kuenning. Paraid: A gear-shifting poweraware raid. In Proceedings of File And Storage Technologies (FAST), 2007.
[27] J. Wilkes, R. Golding, C. Staelin, and T. Sullivan. The
hp autoraid heirarchical storage system. In Proceedings
of ACM Transactions on Computer Systems (TOCS),
1996.
[28] E. Woollacott. Digital content doubles every 18
months. TG Daily, May 19 2009.
[29] Q. Zhu, Z. Chen, L. Tan, and Y. Zhou. Hibernator: helping disk arrays sleep through the winter. In Proceedings
of the twentieth ACM Symposium on Operating Systems
Principles (SOSP), 2005.

14

2011/2/8

