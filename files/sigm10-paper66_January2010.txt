BiFocals: Transparent Instrumentation for
Reproducible Network Measurements

ABSTRACT
We introduce BiFocals — transparent instrumentation that
enables reproducible optical network measurements and systematically characterizes the source, degree, and propagation of error. BiFocals achieves highly accurate network
tomography by leveraging the combination of a precisely
calibrated external hardware timebase from an oscilloscope
with our post-processing software modules. We demonstrate the resulting metrological precision, and associated
reproducibility and reliability, by empirically characterizing
10 Gbps Ethernet packet flows in flight across a short fiberoptic link in a laboratory setting. In contrast with many
common network tomography methods that use software
applications on link endpoints to observe network flow and
infer behavior of network packets in flight, BiFocals enables
reliable in-situ measurements of packets directly on the wire,
without interference with the system under test. BiFocals
thus avoids certain systematic distortions that may otherwise arise, qualitatively altering the distribution of packet
timings. We show that this approach achieves up to six orders of magnitude improved precision in packet delay measurements over many common tomography methods.

1.

INTRODUCTION

The systems and networking communities have always depended upon quantitative measures of network performance,
which play diverse but critical roles in system development
and validation. This work contributes to the science of instrumentation, but with the larger goal of enabling better
understanding of the behavior of high-speed networks and
the applications that utilize them.
We present BiFocals, an instrument for network characterization that stresses reproducibility of results, transparency
of design, and accessibility to fellow researchers (available as
an open-source tool for the research community). No instrument has perfect precision, hence this work carefully controls and quantifies the source, degree, and propagation of
errors in BiFocals itself. Our system obtains highly accurate
in-situ (“in place”) measurements by combining a precisely
calibrated external hardware timebase from an oscilloscope
with a software post-processing stack. The methodology
ensures that the network and the system under observation
are unperturbed by the experimental tool. We demonstrate
BiFocals’ precision, and associated reproducibility and reliability, by observing 10 Gbps Ethernet packet flows in flight,
and contrasting our results with other common endpoint
software techniques for tomography.
We organize this paper along the two primary contribu-

tions of this work: the instrumentation design of BiFocals
and the demonstration of BiFocals in empirically characterizing 10 Gbps Ethernet flows in flight. We first share our philosophy behind BiFocals — providing fellow academic and
industry researchers with transparent and open instrumentation to enable reproducible, reliable, and accurate in-situ
network measurements — before relating BiFocals to past
work on network tomography. We describe the design and
implementation of the BiFocals instrumentation as it transforms analogue oscilloscope sample streams into accurately
time-stamped network packets. We then proceed to apply it
to demonstrate the most precise delay measurements (to our
knowledge) of Ethernet packets in flight, examining 10 Gbps
Ethernet across a short fiber-optic link between commodity
servers in a laboratory setting. We compare these measurements with corresponding ones made with a ubiquitous network tomography technique that employs software services
on endpoints to infer behavior on the wire. Exploring the
effects with different Ethernet controllers, data rates, and
configuration settings, we expose a set of more general network phenomena as well as demonstrate BiFocals’ improvements in accuracy over many common network tomography
techniques. Moreover, in the Appendix, we carefully bound
the error in our BiFocals measurements and share hardware
(oscilloscope and detector) selection criteria.

2.

BACKGROUND AND MOTIVATION

Network tomography is a fertile discipline in which a diversity of experimental and theoretical approaches [9–15,17–
20, 24–26, 29, 32, 33, 35, 36, 38, 39, 42, 44] are used to characterize and understand behavior internal to the network. Although there exists a broad spectrum of network tomography tools and techniques, our work innovates by greatly improving measurement accuracy and eliminating distortions
introduced by the network endpoint, on which traditional
tomographic tools reside. In particular, most experimental
network tomography techniques start by collecting measurements using software that runs on the same commodity machines that are the intended receivers of traffic. A few tools
use more elaborate measurement technologies, but these typically depend on proprietary hardware and software and
hence cannot easily be customized. The compromises between these two extremes include trade-offs reflected in loss
of accuracy, cost, and convenience. Our work enables “baremetal” measurements without requiring any proprietary or
domain-specific technologies, encouraging full customization
and research extensions.
Along this spectrum of network tomography tools, one ex-

treme offers a broad selection of software tools that run on
commodity hardware endpoints or, more generally, pointsof-presence within the network under observation; these include pathChirp [40], CapProbe [22], pathrate [16], pathload
[21], and many others [31, 43]. Such tools typically employ
active or passive probes to observe traffic originating or terminating on the particular endpoints on which the software
is installed. From these endpoint observations, they then infer the behavior of traffic along network paths (potentially
including forwarding elements along the path) in the hope of
revealing some behavior internal to the network. To emphasize their reliance on endpoint hosts, we refer to this entire
family of methods as Machine-Resident Software Tomography (MRST).
Because they infer the behavior of the network from indirect measurements, MRST methods impose a serious risk of
distorting the signals they measure. This distortion results
in measurements that may not be reproducible. Network
flows experience significant transformations between their
transmission link, during their processing through endpoint
software and hardware, to the origination or termination
context of the kernel or userspace processes on a network
device. Low-level details of hardware Ethernet controllers,
system architecture, kernel design, and other factors combine to transform a network flow between the wire and the
MRST measurement.
The mere existence of such distortive transformations does
not itself present an impediment to using MRST methods
at an endpoint to infer packet behavior in flight. However,
we lack a rigorous theory — complete, deterministic, accurate, and specific to the individual hardware and software
stack in use — to account for the full impact of transformations on MRST measurements and inference. Among other
difficulties, characterization would need to account for nondeterminism, non-linearity, “black box” components that
reflect vendor proprietary and intellectual property, complexities in performance optimizations, ASIC (ApplicationSpecific Integrated Circuit) or PCB (Printed Circuit Board)
modification without accompanying external disclosure, and
buggy and incomplete implementations. In light of these intangibles, there are hard limits on what MRST inference can
be used to do.
At the other end of this spectrum of metrology techniques, instruments such as BiFocals capture network traffic
on the transmission media with no distortive transformations. Such capabilities enable in-situ (literally, “in place”)
measurements of network traffic in flight, in contrast to the
ex-situ characterization of MRST methods. In-situ measurements require a precisely calibrated timebase and care
to ensure that the metrology tool does not perturb the system or network under observation.
Our solution is not the first to recognize this limitation
and to work to alleviate it. However, prior work in this space
has largely been proprietary and relatively inflexible: requiring the purchase of a domain-specific hardware measurement tool with limited options for customization. Although
such tools rely on specialized hardware, like FPGA (FieldProgrammable Gate Array) network adapters and ASICs,
they still are subject to distortive transformations simply
because of their basic reliance on conventional 10GBase
PCS (Physical Coding Sublayer), PMA (Physical Medium
Attachment Sublayer), PMD (Physical Medium Dependent
Sublayer), and MAC (Media Access Control) sublayers, not

Motivation

Design Decision

Reproducibility

Use of oscilloscope’s precisioncalibrated hardware timebase

Transparency

Description and distribution of
of software stack (free license)

Accessibility

Reliance only upon basic
features of standard
laboratory equipment

In-situ measurement

Capture of network traffic
without distortive
transformation

Table 1: Motivation for BiFocals instrumentation.
to mention any possible additional processing layers they
may have. However, since these tools do not rely on conventional commodity hardware and software, their transformations are arguably more deterministic and thus potentially
more amenable to mathematical description. For example,
the IXIA interfaces and modules [3] perform time-stamping
transformations within FPGA logic, while the DAG (Data
Acquisition and Generation) Network Monitoring Cards [1]
timestamp packets after receipt in on-chip resident buffers
and before kernel notification (usually via an interrupt) [27].
Recent design proposals [23] to embed network tomography
functionality in routers also belong to this category. Furthermore, established projects have long relied upon external
hardware equipment similar to ours in conjunction with Ethernet testing. In particular, UNH’s InterOperability Laboratory (IOL) manages a members-only Ethernet Consortia
that enables network vendor “plugfests” to promote compatibility [6,7]; while their tests use similar laboratory hardware
as BiFocals, their interests are orthogonal to ours: IOL focuses on assuring IEEE standards compliance, rather than
enabling instrumentation for network research.
Our contribution consists of a new and flexible scheme
for creating customized metrology tools that will not interfere with the system under observation and are not subject
to distortive transformations introduced by system components. BiFocals departs from existing designs of network tomography tools, relying upon a precise external timebase of
an oscilloscope with a post-processing software stack to obtain reproducible in-situ network measurements on which a
rich variety of post-processing techniques can be performed.
We attempt to provide for the larger demand, within the
community, for reproducible and reliable data [30, 34, 41].
Table 1 summarizes the motivations which propel this work.
Nonetheless, we do not believe that BiFocals should replace
existing tomography tools. Despite their relative lack of precision and reproducibility in certain scenarios, MRST techniques have many benefits: simplicity, maturity, diversity,
and efficiency. In comparison, BiFocals incurs significant
computational overheads (far too high for real-time use) and
thus gains its precision and reproducibility at the cost of
more limited data capture capacity. A more detailed exposition of BiFocals’ constraints can be found in Section 3.5. Instead, we argue that BiFocals complements existing MRST
and network tomography techniques in general, all of which
are best applied to different scenarios, and, collectively, form
a spectrum of toolchain choices that balance cost and com-

Figure 1: Experimental setup and instrument design
for BiFocals in-situ measurement of network behavior, showing fiber-optic splitter, photodetector, oscilloscope, and post-processing software stack.
plexity versus precision and reproducibility.

3.

INSTRUMENT DESIGN

This section provides an in-depth discussion of the design of the BiFocals system, with suitable completeness and
pedagogy to allow independent use or implementation. Appendices A and B, address verification and error analysis of
this methodology.

3.1

Overview

Our hybrid hardware and software approach to in-situ
measurements creates what might be considered a SoftwareDefined Optical Network Adapter, analogous to SoftwareDefined Radio (SDR) work, in which hardware and software
are combined to create novel signal processing capabilities.
Here, we use a mixture of software and hardware to create a
flexible tool that performs extraction, measurement, and examination of packets in network flows. Each of these layers
is accessible for future customization.
On an organizational level, the BiFocals instrumentation
system comprises two parts: first, a precisely-timed signal
acquisition device (oscilloscope), which provides an evenly
sampled waveform; and second, a series of post-processing
software modules to extract meaningful network measurements from the sampled waveform. The basic function of
our software toolchain is to accept a vector stream of raw
analog signals from the oscilloscope, extracts the digital bitstream from it, and converts this to a vector stream of packets. Throughout this software processing, we ensure timing
precision by continually maintaining an unaltered reference
between each analog sample, every extracted symbol, and
the external precision timebase of the oscilloscope.
Figure 1 depicts the composition of hardware and software elements of BiFocals instrumentation, as it measures
traffic in flight between two endpoint network devices. To
tap the Ethernet traffic, we insert an optical splitter on the
fiber-optic link connecting the server endpoints. The splitter, a passive fiber-optic device, diverts a portion of the
optical power to our measurement system. The signal to
the receiver network endpoint is unperturbed except for a
slight attenuation in signal strength. As Figure 1 shows, the
tapped signal from this splitter is directed into a photodetector, which transforms modulations in optical power into
variations in voltage. The resulting electrical signal is then
measured by the digital real-time oscilloscope, using analog
to digital converters (ADC’s) to record the voltage of the

incoming signal at a high sampling rate. We take traces
by externally triggering the oscilloscope for single-shot acquisition. These stored waveforms are then retrieved using
remote-communication protocols and the data transferred
to external compute nodes. Finally, off-line post-processing
software modules are used to extract network measurements
of interest.
The software post-processing stack in BiFocals can be decomposed into three primary stages: 1.) Clock recovery
and digitization: Converts sample stream into bitstream;
2.)Packet recovery: Extracts packets from bitstream; and
3.) Payload recovery: Parses packets and analyzes payloads.
In the remainder of this section, we first motivate our use
of an oscilloscope as the foundation of this instrumentation
system and then describe in detail the software stack comprising the above three stages.

3.2

Oscilloscope as Calibrated Timebase

At its core, the external hardware foundation of the BiFocals system is a precisely-timed signal acquisition device,
provided via the oscilloscope. The humble oscilloscope, the
canonical metrological tool to associate signal measurements
with time, is the keystone to the reproducibility, transparency, and accessibility of BiFocals. We leverage almost a
century of experience and expertise in the laboratory application of oscilloscopes to lend validity to the accuracy of our
Bifocals measurement, both through “rules-of-thumb” and
via carefully quantifiable metrics. To better competitively
differentiate themselves, modern digital oscilloscopes have
begun to bundle a significant amount of post-processing capabilities internally — clock recovery, spectral analysis, protocol interpretation, etc. — often calling themselves “Serial
Data Analyzers.” However, since these functions are proprietary to a given oscilloscope vendor and admit no external
review of their implementations, their usage stands in direct
contrast with the degree of accessibility and customizability
that an open software stack like BiFocals intends to provide.
BiFocals deliberately uses the lowest-common denominator
in terms of oscilloscope functionality — namely, the pairing
of a calibrated timebase with ADC’s of appropriate bandwidth and sampling rate. Researchers can thus leverage the
wide availability of basic oscilloscopes without any change on
the part of BiFocals to adapt for specific models or vendors of
oscilloscopes. In fact, our BiFocals implementation has been
tested with oscilloscopes from three vendors — Tektronix,
Agilent (formerly, Hewlett-Packard), and LeCroy. We address the particulars in selection of the hardware foundation
in in Appendix C.

3.3

Influence of Ethernet Standards

While the final stage of our stack — payload recovery —
resembles that of many existing tools, such as wireshark [5],
an understanding of the first two stages — Clock recovery
and digitization and Packet recovery — requires us to discuss some details of the Ethernet Physical Coding Sublayer
(PCS). Our work focuses on the 10 Gbps IEEE 802.3ae standard, examining PCS in the context of 10 Gbps optical fiber
links. We note that BiFocals is designed to measure earlier
optical Ethernet standards as well, which have slight differences in their line codes.
The IEEE 802.3ae standard employs an “NRZ” (NonReturn-to-Zero) data format, where the signal does not return to an intermediary analog position (the “zero” in NRZ)

between pulses, but instead maintains the same analog level
for repeating digital bits. The NRZ data format, while requiring less bandwidth for data transfer, requires a nontrivial clock recovery procedure because the fundamental
clock frequency is missing in its power spectrum [8]. Fortunately, modern Ethernet standards have features designed to
facilitate clock recovery and lock; for example, they ensure
a high occurrence of bit transitions by using encoding tables
or scrambling algorithms and alternating-bit sync headers.

3.4

Details of Software Stack

As introduced in Section 3.1, the BiFocals software stack
can be divided into three primary stages of processing. This
section dissects those three stages and examines their underlying nine individual software modules, as depicted in
Figure 2.

3.4.1

Extract Bit Period

As an initial step to clock recovery, the first module of the
software stack estimates the bit period of the sample stream.
The bit period defines the separation in time, in numbers of
samples from the oscilloscope, between consecutive bits in
our captured trace. The goal of clock recovery is to obtain
an accurate value for this bit period, as it represents the time
between clocking events. The bit period (in samples/bit) Tb
is related to the fundamental repetition rate (in bits/s) ft
of the transceiver, through the sampling rate (in samples/s)
fo of the oscilloscope,
Tb =

fo
,
ft

(1)

where ft reflects the number of bits per second the
transceiver can encode onto the link. Since fo is specified
by the oscilloscope, determination of the signalling speed ft
of the transceiver allows us to perform clock recovery by using the derived bit period, above. For 10GBase-R, the IEEE
802.3ae standard specifies a symbol rate of 10.3125 GBaud,
to within 1 MBd [2]. However, it is important to extract
the actual symbol rate of our signal, rather than relying on
the nominal rate, as a deviation of 1 MBd would introduce
errors every ∼ 10 000 bits.
To extract the true symbol rate of our captured signal,
we first calculate the derivative of the detected signal, which
turns level transitions (edges of bits) into pulses. With this,
extracting the symbol rate of the signal is then simply a matter of finding the peak of its power spectrum. Figure 3(a)
depicts a power spectrum of the derivative of the signal (logarithmic vertical scale); we see a very sharply defined peak
30 dB above the background. Here, we are exposing the
frequency of level transitions in terms of the number of bits
per sample; its inverse is the typical definition of a bit period
(samples per bit). This definition applies very readily to the
next stage of software processing, during which we compute
a metric to judge the quality of this bit period.

3.4.2

Compute Edge Metric

This module seeks to determine the quality of the extracted bit period for use in clock recovery. The bit period calculated in the previous step informs us of the (likely
non-integer) number of samples between successive clocking
events, which refer to the times at which the analog values
in the input sample stream are digitized into bits. However,
due to the finite size of our acquired data traces, the ac-

curacy of the determined symbol rate is limited to 1/NSa ,
where NSa is the number of samples in the captured trace;
by extension, the calculated bit period, Tb , is accurate to
Tb2 /NSa . To understand the consequences of errors in bit
determination, it is helpful to examine the temporal shape
of an individual bit. For each bit, or unit of information
(UI), being transmitted, the 802.3ae standard defines timing allowances for both the rising and falling edges to be
approximately 30% of the bit period, leaving only 40% to
the level state (1/0 bit) [2]. We must take care to only clock
our signal during this intermediary section in which subsequent digitization is unambiguous. Any clocking events that
coincide with the rising or falling edges of the bit can result
in errors when the analog sample is digitized.
Two factors affect the correct alignment of clocking events
with the valid portion of the UI: first, proper determination
of the bit period; second, determination of the initial position at which to apply the clock pulse. To decide this, we
formulate a metric to quantify the intersection of clocking
events with rising and falling edges and examine this metric
as a function of timing offsets from the start of our sample
stream. Figure 3(c) depicts this metric: we apply a polynomial fit to find its minimum, corresponding to the best
alignment between clocking pulses and UI plateaus. We now
have a valid determination of timing location for the initial
clocking event and for all subsequent clocking events.
Unfortunately, the bit period that we extracted earlier
from the spectral density does not actually have sufficient
precision to ensure that the clocking events are properly
aligned with the UI plateau for the entire trace. As described above, the finite sample size limits the bit period accuracy, which in turn can only guarantee proper timing for a
portion of the captured trace. We thus formulate an edge coincidence metric, to detect such deviations during which the
clocking events coincide with rising or falling edge. (Technically, we define this metric as the percentage of clocking
events, over a certain fractional time interval, that result in
analog sample values lying in the middle 40% of the amplitude range between the mean “low” values and the mean
“high” values.) Figure 3(d) shows this edge coincidence metric for the initial clock recovery shown in Figure 3(b) using
the bit period extracted above.

3.4.3

Determine Beat Frequency

We immediately notice that the edge coincidence metric
is not a random picture of misalignment of clocking events
— instead, it shows clear peaks separated by long periods
of proper alignment of clocking events. It is exactly as we
understand: an initially perfectly-clocked bitstream will depart from this proper timing due to the inaccuracy in symbol
rate. The peaks in the edge coincidence metric depict the
gradual accumulation of small errors in the bit period such
that the position of clocking event slowly moves across the
UI, off of the plateau (where the metric is zero) and then
across each edge (where the metric peaks at the midpoint of
each edge). The frequency at which complete walkoff from
perfect bit clocking occurs is precisely the beat frequency between the symbol rate determined in previous steps and the
true symbol rate. This process, shown as the fitted curves
in Figure 3(d), provides a beat frequency that we use to
estimate the bit period of the new clock.

3.4.4

Estimate New Clock

Figure 2: Flow-chart depicting three stages (and underlying nine modules) of post-processing software stack:
Clock recovery and digitization; packet recovery; and payload recovery. Distinct colors and shapes denote
different stages.
In this fourth module, we apply the principles behind beat
frequencies to obtain an estimate for the actual frequency
(fe ) from the initial (fi ) and the beat frequency (fb ):
fe = fi ± fb

(2)

Using these two new frequency estimates, we calculate their
associated bit periods and iterate through the “Compute
Edge Metric” step. One of these two bit period estimates will
typically show more peaks in the resulting edge coincidence
metric than the original estimate, while the other will have
fewer (or no peaks) across the entire waveform trace. We automate the determination of the better bit period estimate
by comparing the integrals of these edge coincidence metrics,
which reflect the aggregate clocking-event–edge-coincidence
across the entire waveform trace. Generally, these integrals
will differ by many orders of magnitude after a single iteration; for example, after only the first iteration through clock
recovery, we found that the integral of the preferred bit period estimate was at least six orders of magnitude less than
that of the other in 80% of our clock recovery attempts in
Section 4. (In fact, this type of iterative refinement of the
bit period is analogous to a hardware clock recovery circuit.
In hardware clock recovery, typically a phase-locked loop
(PLL) is used in conjunction with a voltage-controlled oscillator (VCO); the VCO providing a “seed” frequency and the
PLL providing the feedback to sync the recovered clock to
the actual data.)

3.4.5

Digitize Stream

With a bit period that is known to be aligned across the
entire trace (tested for traces as large as 100 MSamples of
input) and skew due to initial clock offset minimized, we can
now digitize the analog sample stream from the scope and
create a digital bitstream vector. Thus, the first of three
stages of the software stack is complete.

3.4.6

Synchronize Frame

We turn to the conversion of this bitstream into a network packet stream, which begins with the sixth module in
our software stack. With access to the bitstream, it now
becomes meaningful to discuss the impact of the 64b/66b
line code in this process. This module seeks to synchronize the two-bit frame headers of this 64b/66b line code.

As explained above, the 10GBase-R PCS standard specifies that only “01” and “10” are valid two-bit frame headers.
Thus, for each of the first 66 bit positions in the stream,
we simply examine the correlation of level transitions, separated by the length of the frame, along the entire trace.
(Again, our software process is very closely aligned with the
hardware implementation, in which continued misalignment
of subsequent frames forces the hardware demultiplexor to
cycle-slip and continue the search for a valid synchronization
position.) Figure 3(e) demonstrates the synchronization of
a stream and the resulting extraction of the sync / frame
header bits.

3.4.7

Descramble Frame

The seventh module in our software stack provides for the
descrambling of the 64b/66b line code specified in the Physical Coding Sublayer (PCS) of the IEEE 802.3ae standard
of 10GBase-R. PCS wraps 64 bits of “symbols” in a 66-bit
frame (for an overhead of 3.125%): the 64 bits are 8 octets
of data/control information, while the two bits that delineate the frame ensure a signal transition every frame (as
only “01” and “10” are allowed, while “00” and “11” result in
errors), thus easing clock recovery. Upon transmission, the
64-bit portion of the frame is fed through a multiplicative
self-synchronizing scrambler, defined by the polynomial,
G(x) = 1 + x39 + x58 ,

(3)

After 64b/66b frame synchronization in the previous step,
this module then descrambles the 64b/66b frame payload
from the bitstream, using the descrambler depicted in Figure 4. G(x) ensures that the resulting signal has desired DCbalance characteristics that are well-aligned with the physical fiber-optic transmission media, irrespective of transmitted data. Self-synchronization ensures that our descrambler
does not require existence or knowledge of any given initial
state to implement G(x). Indeed, our module synchronizes
itself after a single 64b/66b frame and, thereafter, produces
deterministic output.

3.4.8

Decode Frame

The eighth software module transforms the descrambled
bitstream vector into a vector of network packets, by removing and interpreting the symbols that are inserted into the

Figure 3: Clock recovery and digitization stage within BiFocals software stack. (a) shows the Fourier transform
of the time derivative of our signal with the sharply defined peak revealing the fundamental frequency, which
is used to compute the bit period. (b) depicts a representative waveform trace, showing raw analog samples
from the scope, samples at clocked intervals, and resulting digitized values. (c) demonstrates the minimization
of overlap between clocking events and rising or falling edges of bits to optimize the phase of the clock with
the waveform. (d) displays the edge coincidence metric; peaks correspond to misalignment of clocking events.
(e) shows the extraction of the offset position of the 64b/66b PCS sync / frame header in the bit stream by
correlating bit transitions separated by the frame’s 66-bit length.

Figure 4: Representation of self-synchronizing descrambler, with G(x) = 1 + x39 + x58 polynomial, from
64b/66b Physical Coding Sublayer of IEEE 802.3ae
standard.
packet stream by the 64b/66b PCS layer. The most important of these are those control codes signifying the “Start
of Packet” or “Termination of Packet” at given bit values,
along with “Idle” or “Error” control codes, and the actual
data bits. We continue to preserve the reference time-base
as we perform this final packet extraction, by mapping each
extracted packet to a given bit (and thus analogue sample)
value of the intermediate and original vectors, respectively.

3.4.9

Parse Payload

Finally, in the last phase of our software post-processing,
we parse the Ethernet packets and extract the full headers
and payloads for all contained packets. Typically, as in this
work, such parsing would provide Ethernet, Internet Protocol (IP), and either Transmission Control Protocol (TCP) or
Uniform Datagram Protocol (UDP) headers and payloads.
In conclusion, after significant computational effort, we
have transformed a raw analog sample stream from the
ADC’s of our oscilloscope into a meaningful capture trace of
packets, with a carefully associated precision timebase.

3.5

Constraints

Throughout Section 2, we motivated BiFocals as providing transparent instrumentation for extremely precise network metrology, that would complement existing MachineResident Software Tomography (MRST) techniques, each
occupying a different place on the spectrum of tools of varying accuracy and “cost”. In that vein, while presenting the
design and implementation details of BiFocals, we believe it
also important to discuss its fundamental constraints.
The primary constraint inherent in BiFocals is that it is
not designed to provide real-time access to the network measurements it makes. As is surely obvious through the previous discussion of our software stack, additional time (beyond the delay between packets at Ethernet line speed) is
required to accomplish such post-processing, to extract network packets, and to analyze their characteristics. Clearly,
most MRST methods allow real-time, or near real-time,
measurements, but rely upon network adapter hardware at
the endpoints to accomplish the packet extraction that we
are doing in software. As described extensively in Section 2,
BiFocals trades the ease of this real-time acquisition for insitu measurement of traffic in flight with no distortive transformations.
Another important constraint of BiFocals reflects the underlying oscilloscope technology. The memory depth of digital scopes defines the upper bound on the number of contiguous packets we can observe and measure. The combination
of scope sampling rate and scope memory depth provide for

a maximum wall-clock time that can be captured in a single acquisition, irrespective of the actual density of packets
on the wire. The number of contiguous packets captured
off the optical wire in that period of time is thus bounded.
(Significant numbers of contiguous packets can still be examined: for an example with 10GBase-R Ethernet, a 40
GSa/sec scope with 1 GSample of memory can capture over
20 000 UDP packets, of 1500-byte MTU, or over 750 000 raw
IPv4 packets, of minimum 20-byte payload.) Furthermore,
between each single-shot acquisition into scope memory storage, all scopes have an intermediary period, known as “deadtime” or “re-arm time”, during which they are unable to trigger and acquire additional traces. Modern high-performance
scopes vary widely in the percentage of dead-time during
deep memory captures: 75% dead-time is considered outstanding, while 99.9% dead-time is still common on many
otherwise very able scopes. For situations when large numbers of packets are desired, while we cannot continuously
acquire packets because of limited memory depth and finite dead-time of the oscilloscope, statistical results can be
made from ensembles of contiguous packets. We have implemented, and include as part of the BiFocals instrumentation,
remote client libraries to interface with vendor scopes and
automate the process of single-shot acquisition and transfer
of scope traces to compute nodes for post-processing.

4.

DEMONSTRATION OF BIFOCALS

In this Section, we demonstrate the application of BiFocals to in-situ measurement of network packet delay on
10 Gbps Ethernet over fiber-optic links, contrasting our results with those obtained using a common implementation
of Machine-Resident Software Tomography (MRST) that we
tuned for improved precision. This demonstration supports
our initial claim that BiFocals delivers five to six orders of
magnitude increased precision over common MRST techniques.

4.1

Experimental Ensembles

For these experiments, we examine two different Ethernet
controllers, as well as various data rates and kernel driver
configurations, to demonstrate that our results are not specific to one particular configuration. We examine 10GBaseLR network adapters from both Intel and Myricom: Intel
10 Gigabit XF LR Server Adapter (with Intel 82598EB controller and ixgbe driver version 1.3.47) at both 2000 Mbps
and 200 Mbps, and Myricom Myri-10G Network Interface
Card (with Myricom Lanai Z8ES controller and myri10ge
driver version 1.5.1) at 2000 Mbps. As we discuss in Section 4.4, certain kernel driver configurations were unable to
fully support the 2000 Mbps nominal data rate.
We experimented with various kernel driver settings for
two key parameters: Linux NAPI (“New API”) [4] support
and device support for “Interrupt Throttling” or “Interrupt
Coalescence”. Both are batching techniques that improve
overall throughput during high network traffic load. Interrupt throttling is an implementation specific to each Ethernet controller, while NAPI is a technique used by the kernel
network stack, provided that the Ethernet controller allows
such functionality. With interrupt throttling enabled, the
network adapter attempts to moderate processor load by
accumulating multiple packet events over a specified time
period (generally ∼ 10–1000 microseconds) before interrupting the processor and delivering all those events at once.

Ethernet
Controller
Intel 82598EB
Intel 82598EB
Intel 82598EB
Intel 82598EB
Myri Lanai Z8ES
Myri Lanai Z8ES
Intel 82598EB
Intel 82598EB

Data rate NAPI Interrupt Packets
[Mbps]
Throttling
[#]
2000
2000
2000
2000
2000
2000
200
200

Yes
Yes
No
No
Yes
Yes
Yes
No

Yes
No
Yes
No
Yes
No
Yes
No

18962
22347
18040
16181
18864
17400
5474
5473

Table 2: Ensembles of various Ethernet controllers,
data rates, and kernel module configurations, descending rows correspond to Figure 5(a) through
Figure 5(h), respectively.

Likewise, NAPI performs interrupt mitigation in software
by dynamically disabling interrupts and polling the device
at a rate appropriate to the processor. This ensures that
the processor does not enter “receive-livelock” [28] and that
packets are dropped as early as possible so that the least
amount of effort is expended processing them. The Intel
adapter allows for user configuration of NAPI and interrupt throttling, while the Myricom adapter allows interrupt
throttling to be configured but forces incorporation of NAPI
in the driver source.
Table 2 enumerates the eight ensembles of different measurements for various Ethernet controllers, data rates, and
kernel module configurations, and lists the total number of
captured packets for each ensemble (extracted from ∼ 45
traces for each 2000 Mbps ensemble and ∼ 130 traces for
each 200 Mbps ensemble).

4.2

Specification of Equipment

In Section 3, we presented BiFocals without reference to
any particular models of oscilloscopes or photodetectors, as
the system is agnostic to such details. Here, we specify the
particular implementation of our network endpoints and BiFocals hardware. All measurements are conducted in a laboratory setting with Ethernet traffic flowing between two Dell
PowerEdge R900 enterprise servers. Running Linux 2.6.27.2
kernels, these servers are based upon the Intel 7300 chipset,
with 16 cores, 32 GB of memory, and an 8-lane PCI Express
bus (2.5 GT/sec per lane) for the network adapter. These
network adapters transmit at an optical wavelength of 1310
nm (10GBase-LR) across a 3m length of singe-mode fiber
(SMF), connecting the servers.
We tap the fiber-optic link with a NetOptics TP-LX5SCSLM fiber-optic splitter, diverting 50% of the optical
power into a Discovery Semiconductor DSC-R402 photodetector for conversion into electrical signal. We acquire
100 MSample traces on a single 11 GHz channel of a LeCroy
SDA 11000-XL real-time digital oscilloscope at a sample rate
of 40 GSa/sec and clock accuracy 1 part per million, delivering analog voltage measurements every 25 picoseconds for
a total contiguous period of 2.5 milliseconds. We gather an
ensemble of individual traces to obtain the number of packets enumerated in Table 2. We then correlate each packet
obtained with BiFocals to the same packet as measured by
the MRST toolchain.

4.3

Comparison MRST Implementation

As a representative MRST implementation, we select the
iperf [43] software tool and use UDP flows to characterize network traffic on one of our endpoints and infer packet
behavior on the wire. We expended significant effort improving iperf to ensure that our implementation reflects a
best effort at capturing the the precision and reproducibility
of an exemplar MRST method.
The most common short-coming in the utilization of software tomography tools involves the means by which packets
are time-stamped as they leave the sender and arrive at the
receiver. The most recent upstream release of iperf (version
2.0.4) accomplishes such time-stamping strictly in userspace
and only with the stock gettimeofday() call. The lack of
precision of gettimeofday() is well recognized by Linux developers, serving as the genesis for its replacement by POSIX
timers (clock_gettime()) that may support nanosecond
resolution, depending on the kernel support for high resolution hardware clock sources (e.g. hrtimers and HPET).
Alternatively, if an absolute wall-clock value is not required, one can use processor specific instructions that count
the number of cycles since reset. For example, all x86 processors since the Intel Pentium have included a 64-bit time
stamp counter register that can be read by the RDTSC instruction. The value returned can be normalized based on
the speed (cycles per second) of the processor. We chose
this approach as it provides more accurate timestamps, due
to the simplicity of the simple register read (RDTSC) over
the issuance of a system call (clock_gettime()) to lock and
read the wall clock. (As a matter of fact, the wall clock is
only updated periodically, hence reading the wall clock with
nanosecond precision involves additionally reading a high
resolution hardware clock source and computing a delta that
must be added to the stored wall clock value.)
A number of potential issues still need to be overcome
to maintain the accuracy of this method. Modern Intel superscalar architectures rely on out-of-order execution of instructions, and the RDTSC instruction is not serializing on all
micro-architectures (e.g. on Core2). Due to deep speculation on such processors, the RDTSC instruction may not be as
accurate as possible — depending on the micro-architecture,
it could have errors in the tens of processor cycles. Further, power-saving techniques that are now ubiquitous can
place processors in sleep states, thereby lowering their speed
(cycles per second) and further obfuscating the meaning of
these timestamp counters. Finally, modern SMP operating
systems can schedule a given process on alternative processors and thus confuse subsequent readings of the timestamp
counter (since the cycle-count registers are not synchronized
between processors).
In implementing CPU Time Stamp Counters for software
tomography and embedding RDTSC calls into iperf, we address each of these potential problems. First, we serialize
all instructions before our RDTSC calls, by inserting explicit
memory barriers. Next, we explicitly disable all sleep states
for the machines. Thus, for the iperf sender, we replace
calls to gettimeofday() with assembly calls to RDTSC and
we bind the iperf task / process to execute exclusively on
CPU 0, thereby preventing the kernel from scheduling the
iperf task on any other core. More importantly, for the
packet timings on the iperf receiver process, we slightly
modify the stock Linux kernel to “piggyback” a value returned by a call to RDTSC on the SO_TIMESTAMP socket con-

trol message. If the SOL_SOCKET / SO_TIMESTAMP socket option is enabled, the kernel will time stamp each received
packet at an early point on the receive path — typically
after the interrupt service routine or during NAPI polling
(i.e., within the netif_rx or the netif_receive_skb kernel
routines). This enables iperf, and in general any socket,
to extract more accurate timing information of packets received. Since the in-kernel early packet time stamps are
taken within the network stack which executes in softirq context on the core that received the interrupt notifying packet
arrival, we explicitly set the interrupt SMP affinity of the
network adapter (/proc/irq/<irq #>/smp_affinity) to associate with a single processor. Additionally, we disable the
irqbalance daemon that attempts to dynamically load balance IRQ requests across many cores.
Finally, we can ignore time synchronization between endpoints, as all timing measurements are referenced only to
others on the same endpoint in the form of packet delay.

4.4

Description of Measurements

With both BiFocals and iperf techniques, we obtain a
contiguous trace of captured UDP packets, with packets and
their payloads completely parsed, either directly off the fiber
via BiFocals or with the userspace iperf software on the receiver endpoint. We directly correlate each packet captured
by BiFocals with the same one captured by our custom version of iperf, using the unique packet identification number
embedded into each payload by the iperf sender.
Every Ethernet packet in the network flow also has an associated time-stamp, provided independently by BiFocals in
flight and the iperf MRST toolchain on the receiver endpoint. We first verify successful BiFocals capture of packets
with Ethernet CRC, as discussed in Appendix A; to provide the most advantageous consideration to our comparison
MRST method, we then discard any BiFocals measurements
during which iperf observes packet loss or reordering on the
receiver endpoint. We then compute the time delay between
consecutive packets for both methods. The differential between each time delay for the same pair of packets, as measured separately by BiFocals and iperf, represents the loss
in precision associated with the use of MRST methods to
infer behavior of packets in flight.
Each subfigure in Figure 5 presents results for one of the
eight ensembles listed in Table 2, examining the differentials
in time delays between BiFocals and iperf measurements.
For each ensemble, the histogram of packet delay differentials provides statistical insights into the time distribution
for all packets. On the horizontal axes, negative values of
this differential imply greater delay measured by BiFocals
compared to iperf; the vertical axes, with logarithmic scale,
reveal the secondary structure of this histogram that cannot
be characterized simply by a Gaussian distribution (with
mean of µ and variance of σ 2 ). The top-left inset for each
subfigure shows a small portion of the raw results for the
BiFocals–iperf differentials in packet delay. The top-right
inset uses Welch’s average periodogram method to compute
the spectral density of the packet delay differential, with
frequencies expressed in inverse packet number.
These results can alternatively be interpreted in two ways:
1. Representing a statistical characterization of the distortive transformation, as introduced in Section 2, inherent in the particular endpoint hardware and software for a given ensemble (e.g., buffer and Direct Mem-

ory Access (DMA) ring of network adapter, kernel
driver settings, kernel network stack execution contexts, socket buffers, etc.)
2. Depicting the error inherent in the interaction between
MRST (iperf) tools with the system under observation and the interpretation of raw MRST (iperf) measurements as applicable to behavior of network packets
on the fiber in flight.
Figure 5(a) shows results for 2000 Mbps UDP flows between Intel 10 Gigabit XF LR adapters configured with
NAPI and interrupt throttling. We observe a histogram with
multiple peaks, the highest at -3 µs delay, with offsets to 24 µs and 14 µs and a broad secondary peak around 90 µs.
Inset, we see the raw packet delay differential data, showing significant delay events of up to 100 us, between BiFocals and MRST (iperf) measurements, every 25-50 packets.
Also inset is the periodogram showing peaks at low frequencies between 0.04-0.13 Hz [1/packets], corresponding to the
increased delay differential every 25-50 packets. The packet
behavior of the same Intel 10 Gigabit XF LR adapter, this
time with NAPI disabled and interrupt throttling enabled,
is shown in Figure 5(b) and largely resembles the situation
with both NAPI and interrupt throttling enabled, with the
exception of a single larger peak in the power spectrum,
at 0.09 Hz, rather than a number of peaks in a range centered at that value. Figure 5(c) examines the same network
adapter with NAPI enabled, but now with interrupt throttling disabled: This histogram shows a tight distribution
between -10 and 16 µs, with a peak at 0 µs; the variations
in raw packet delay differentials are much smaller than the
previous results, with a peak in the power spectrum at mid
frequencies (0.41 Hz). This scenario is also almost identical
to the case when both NAPI and interrupt throttling are
disabled, depicted in Figure 5(d).
Next, we review the results for measurements on 2000
Mbps network traffic between Myricom Myri-10G network
interface cards, which enforces NAPI processing through
driver source code. Figure 5(e) shows results with interrupt throttling, with a central peak at -3 µs, and a broad
secondary structure at ∼ 50 µs. Variations, of up to 50 µs, in
the packet delay differentials are visible every 10 packets or
so; significant structure is seen in the spectral density with
evenly spaced peaks at 0.15, 0.30, and 0.45 Hz (and smaller
peaks at 0.60 and 0.75). For interrupt throttled disabled,
Figure 5(f) depicts a central peak at 0 µs of ∼ 31µs width,
and an unexplainable narrow secondary peak at 32 µs; there
are negligible variations in the raw packet delay differentials
which translate into a complete lack of structure in the periodogram.
Finally, we review the results of the same Intel 10 Gigabit
XF LR adapter with Ethernet traffic flowing at 200 Mbps.
As seen in Figure 5(g), measurements with both NAPI and
interrupt throttling engaged result in a histogram with a
stark bimodal distribution with relatively narrow peaks at
-54 and 63 µs, of width 37 and 50 µs, respectively. The
results of raw packet delay differentials show variations up
to almost 100 µs between every packet. This is similarly
reflected in a large peak in the power spectral density at
0.93 Hz, with secondary structure at 0.14 Hz. For 200 Mbps
flow between the same adapters, the results differ greatly
when NAPI and interrupt throttling are disabled, such as
in Figure 5(h). Here, we note a tight unimodal histogram

Figure 5: Comparison of BiFocals and MRST by Ethernet packet delay differential: delay differentials computed between BiFocals in-situ measurement of packets in flight on fiber and inferences of same values with
MRST techniques on receiver endpoint. Subfigures show histograms of BiFocals–MRST delay differentials
[µs] for eight ensembles (corresponding to measurements with different Ethernet controllers, data rates, and
kernel driver settings, as enumerated in Table 2). Insets show for each ensemble: upper left, raw delay differentials [µs] as a function of time [packet #] for representative flows; upper right, periodogram of spectral
components of differentials, showing peaks in differential power as a function of frequency [1/packet].

distribution with a peak at 0 µs offset and a width of 33 µs.
The raw packet delay differential is practically constant, and
the associated power spectral density is completely devoid
of structure.

4.5

Discussion

The results above show a number of interesting features.
Irrespective of Ethernet controller and bandwidth, they provide unmistakable evidence that transformations, between
packets in flight and packets on endpoints, are quite complicated, even statistically. As a result, MRST inferences about
in-flight behavior of packets are non-trivial and very much
reliant upon models of such transformations. We quantify
the error in the above MRST measurements through the determination of packet delay differentials. We find errors of
10–100 µs when compared to our BiFocals measurements.
As justified through an error analysis of BiFocals in Appendix B, BiFocals’ accuracy exceeds that necessary to extract “ground truth” timings based upon nominal 96.970 ps
Units of Information (UI’s) in the IEEE 802.3ae standard [2].
Therefore, the ratio of 10–100 µs errors to UI’s of ∼ 100ps
provides our claimed improvement in precision of five to six
orders of magnitude over typical MRST techniques.
In fact, we recognize a clear trend that interrupt coalescence or throttling dominates the distribution and structure
of these results. We can actually correlate the secondary
structures that result from interrupt throttling on the Intel
and Myricom 10GBase-R cards. Even though such structures are centered at two values for packet delay differential
— ∼ 90µs for Intel and ∼ 50µs for Myricom — these correlate within 10% with the two different default interrupt
coalescence timeout values we used here (125 µs for Intel
and 75 µs for Myricom).
Interrupt throttling has even greater effect at the lower
data rates we measured on the Intel 10 Gigabit XF LR card.
Significant structure is imposed in the periodogram for 200
Mbps flow on an example of this adapter with both NAPI
and interrupt throttling enabled, especially in comparison to
the almost constant spectral density and raw packet delay
differentials of the adapter with NAPI and interrupt throttling disabled.
For similar kernel driver configurations of NAPI, and normalized for timeout values in interrupt throttling, we observe that Intel and Myricom adapters exhibit similar statistical properties in their histograms and spectral densities.
In conclusion, there are a number of lessons to be drawn
here by relating the precise results from BiFocals with other
less computationally-costly MRST methods. First, as network performance increases during a period of relatively
flat processor clock speed growth, techniques such as interrupt throttling will continue to pervade the networking
environment to prevent packet loss on endpoints and artificial depression of maximum data rate. Yet, these techniques greatly complicate typical applications of and inferences from MRST tools. For just such reasons, among others, BiFocals should complement existing tomography techniques, allowing for reproducible and highly precise measurements.

5.

CONCLUSIONS

Our work responds to the recognized need for greater
reproducibility in systems and network measurements and
presents our BiFocals system of transparent instrumenta-

tion. BiFocals enables reproducible in-situ network measurements, strict characterization of measurement error,
transparency into the metrological tool chain, and accessibility of hardware and software to fellow academic and
industry researchers. This work provides two contributions to the field: First, we carefully describe the design,
function, verification, and error analysis that allows for reproducible in-situ measurements using our BiFocals instrumentation, which we distribute to the community under a
Free Software license at http://<redacted for blind review>/. Second, to the best of our knowledge, we achieve the
most precise measurements to date of delay characteristics
of 10GBase-R Ethernet packets in flight. In contrasting our
results with those from Machine-Resident Software Tomography (MRST) methods, we find errors on the order of tens
to hundreds of microseconds from such MRST techniques —
up to six orders of magnitude greater than the uncertainty
in BiFocals measurements of such packet flows. Further, we
present detailed histograms and spectral densities of these
errors for measurements of different Ethernet controllers, as
well as for various kernel module settings and data rates.

6.

REFERENCES

[1] DAG Network Monitoring Cards. http://www.
endace.com/dag-network-monitoring-cards.html.
[2] IEEE 802.3-2008 – Section Four. http:
//standards.ieee.org/getieee802/802.3.html.
[3] IXIA Interfaces - 10 Gigabit. http://www.ixiacom.
com/products/interfaces/index.php.
[4] NAPI.
http://www.linuxfoundation.org/en/Net:NAPI.
[5] Wireshark. http://www.wireshark.org/.
[6] 10 Gigabit Ethernet Consortium of the University of
New Hampshire InterOperability Laboratory.
10GBASE-R PCS Testing System (Megalodon).
http://www.iol.unh.edu/services/testing/
ethernet/tools/megalodon/.
[7] 10 Gigabit Ethernet Consortium of the University of
New Hampshire InterOperability Laboratory.
10GBASE-X PCS Testing System (T-Rex).
http://www.iol.unh.edu/services/testing/
ethernet/tools/trex/.
[8] G. P. Agrawal. Fiber-Optic Communication Systems.
John Wiley & Sons, Inc., New York, 3rd edition, 2002.
[9] K. G. Anagnostakis and M. Greenwald. cing:
Measuring network-internal delays using only existing
infrastructure. In IEEE INFOCOM, 2002.
[10] K. G. Anagnostakis and M. Greenwald. Direct
measurement vs. indirect inference for determining
network-internal delays. Performance Evaluation,
49:1–4, 2002.
[11] J.-C. Bolot. End-to-end packet delay and loss behavior
in the internet. In SIGCOMM ’93.
[12] T. Bu, N. Duffield, F. L. Presti, and D. Towsley.
Network tomography on general topologies.
SIGMETRICS Perform. Eval. Rev., 30(1):21–30, 2002.
[13] T. Bu, T. Friedman, J. Horowitz, D. Towstey,
R. Caceres, N. Duffietd, F. L. Presti, S. B. Moon,
S. Atl, and V. P. Actr. The use of end-to-end multicast
measurements for characterizing internal network
behavior. IEEE Comm. Mag., 38:152–159, 2000.

[14] M. Coates, A. Hero, R. Nowak, and B. Yu. Internet
tomography. IEEE Signal Processing Magazine,
19:47–65, 2002.
[15] M. Coates and R. Nowak. Network loss inference using
unicast end-to-end measurement. In Proc. ITC Conf.
IP Traffic, Modeling and Management, 2000.
[16] C. Dovrolis, P. Ramanathan, and D. Moore.
Packet-dispersion techniques and a
capacity-estimation methodology. IEEE/ACM Trans.
Netw., 12(6):963–977, 2004.
[17] N. Duffield. Simple network performance tomography.
In ACM SIGCOMM Internet measurement (IMC’03).
[18] N. Duffield, F. L. Presti, V. Paxson, and D. Towsley.
Network loss tomography using striped unicast probes.
IEEE/ACM Trans. Netw., 14(4):697–710, 2006.
[19] N. G. Duffield, J. Horowitz, F. L. Presti, and D. F.
Towsley. Network delay tomography from end-to-end
unicast measurements. In IWDC ’01: Proceedings of
the Thyrrhenian International Workshop on Digital
Communications, pages 576–595, London, UK, 2001.
[20] R. Govindan and V. Paxson. Estimating router ICMP
generation delays. In Proceedings of PAM, 2002.
[21] M. Jain and C. Dovrolis. End-to-end available
bandwidth: measurement methodology, dynamics, and
relation with TCP throughput. IEEE/ACM Trans.
Netw., 11(4):537–549, 2003.
[22] R. Kapoor, L.-J. Chen, L. Lao, M. Gerla, and M. Y.
Sanadidi. CapProbe: a simple and accurate capacity
estimation technique. SIGCOMM Comput. Commun.
Rev., 34(4):67–78, 2004.
[23] R. R. Kompella, K. Levchenko, A. C. Snoeren, and
G. Varghese. Every microsecond counts: tracking
fine-grain latencies with a lossy difference aggregator.
In Proceedings of SIGCOMM ’09.
[24] K. Lai and M. Baker. Measuring link bandwidths
using a deterministic model of packet delay.
SIGCOMM Comp. Comm. Rev., 30(4):283–294, 2000.
[25] R. Mahajan, N. Spring, D. Wetherall, and
T. Anderson. User-level internet path diagnosis.
SIGOPS Oper. Syst. Rev., 37(5):106–119, 2003.
[26] Y. T. Mark, A. Tsang, M. Coates, and R. Nowak.
Passive network tomography using em algorithms. In
Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, 2001.
[27] J. Micheel, S. Donnelly, and I. Graham. Precision
timestamping of network packets. In IMW ’01.
[28] J. C. Mogul and K. K. Ramakrishnan. Eliminating
receive livelock in an interrupt-driven kernel. ACM
Trans. Comput. Syst., 15(3), 1997.
[29] S. B. Moon, P. Skelly, and D. F. Towsley. Estimation
and removal of clock skew from network delay
measurements. In INFOCOM ’99, pages 227–234.
[30] T. Mytkowicz, A. Diwan, M. Hauswirth, and P. F.
Sweeney. Producing wrong data without doing
anything obviously wrong! In ASPLOS ’09.
[31] N. Spring and D. Wetherall and T. Anderson.
Scriptroute: A Public Internet Measurement Facility.
In 4th USENIX Symposium on Internet Technologies
and Systems, 2002.
[32] H. X. Nguyen and P. Thiran. Network loss inference
with second order statistics of end-to-end flows. In

Proceedings of the 7th ACM SIGCOMM IMC ’07.
[33] V. N. Padmanabhan, L. Qiu, and H. J. Wang. Passive
network tomography using bayesian inference. In
Proceedings of the 2nd ACM SIGCOMM Workshop on
Internet measurment (IMW ’02).
[34] V. Paxson. Strategies for sound internet measurement.
In Proceedings of the 4th ACM SIGCOMM IMC ’04.
[35] V. Paxson. End-to-end internet packet dynamics.
SIGCOMM Comput. Commun. Rev., 27(4):139–152,
1997.
[36] V. E. Paxson. Measurements and analysis of
end-to-end Internet dynamics. PhD thesis, Berkeley,
CA, USA, 1998.
[37] D. Peterson, W.W. Brown. Cyclic codes for error
detection. In Proceedings of the IRE, 1961.
[38] R. S. Prasad, M. Murray, C. Dovrolis, and K. Claffy.
Bandwidth Estimation: Metrics, Measurement
Techniques, and Tools. IEEE Network, 17:27–35, 2003.
[39] F. L. Presti, N. G. Duffield, J. Horowitz, and
D. Towsley. Multicast-based inference of
network-internal delay distributions. IEEE/ACM
Trans. Netw., 10(6):761–775, 2002.
[40] V. J. Ribeiro, R. H. Riedi, R. G. Baraniuk,
J. Navratil, and L. Cottrell. pathChirp: Efficient
Available Bandwidth Estimation for Network Paths.
In Proceedings of the PAM Workshop (’03).
[41] M. Roughan. Fundamental bounds on the accuracy of
network performance measurements. In Proceedings of
SIGMETRICS ’05.
[42] J. Sommers, P. Barford, N. Duffield, and A. Ron.
Improving accuracy in end-to-end packet loss
measurement. In SIGCOMM ’05.
[43] A. Tirumala, F. Qin, J. Dugan, J. Ferguson, and
K. Gibbs. Iperf – The TCP/UDP bandwidth
measurement tool. 2004.
[44] Y. Tsang, M. Coates, and R. Nowak. Network delay
tomography. Signal Processing, IEEE Transactions on,
51(8):2125–2136, Aug. 2003.

APPENDIX
A.

VERIFICATION

In designing transparent instrumentation for reproducible
measurements, we believe it crucial to provide verification
steps to ensure that the results are not only reproducible,
but also reliable. To that end, we have implemented two primary methods of verification: eye diagrams and checksums.
For the first verification method, we ascertain the quality of our received signal by generating “eye diagrams” with
our software stack. This visualization method, so named for
its characteristic shape, is commonly used in the communications community to verify the quality of communications
links. In software, the eye diagram is constructed by using
the recovered clock rate to overlay the bits on top of each
other. The eye diagram, representing the statistical ensemble of many Units of Information (UIs), is probably best used
to infer properties of the transmission link and endpoints
such as signal noise, timing jitter, intersymbol interference,
speed, among other things. We advocate use of softwarereconstructed eye diagrams as the first step in confirming the
technical suitability of the hardware detection components
of the BiFocals system (photodetector and oscilloscope). In
Figure 6, we show eye diagrams corresponding to three different photodetectors we tested (oscilloscope used for A/D
conversion was the same for all three) to perform optical
to electrical conversion of our ethernet traffic. The first eye
corresponds to measurements we made with a Picometrix D8ir photodetector, a high-speed photodetector with response
bandwidth of 50 GHz and rise time of 8 ps. This detector,
despite its high-speed capabilities, is ill-suited for our measurements, as the large bandwidth is not necessary to detect
our 10 Gbps signal, and the low conversion gain yields a
barely open eye, with significant amplitude noise. The next
subfigure shows the eye from the signal retrieved from an
Agilent 83440C photodetector (20 GHz of bandwidth, rise
time 22 ps). The eye is open, indicating a signal that can
easily be thresholded to decode the bit pattern. We observe
some overshoot of the 1 and 0 signal levels, a tell-tale sign
excessive bandwidth in the detection chain. The final subfigure shows the eye diagram drawn from the signal detected by
our detector of choice: the Discovery Semiconductor DSCR402 photodetector. This photodetector is designed for 12.3
Gbps transmission with built-in electrical amplifier, and is
ideally suited off-the-shelf for our 10GBase-R measurements.
(Specifically, as tested by the manufacturer, our DSC-R402
device has a −3 dB bandwidth of 9.6 GHz and a conversion
gain of 428 mV/mW.) The eye is wide open, with minimal
amplitude noise and timing jitter , and no discernible overshoot of 1 or 0 signal levels. The openness of the eye can
be directly associated with the “cleanness” of the acquired
signal and its ability to be decoded into a bitstream without
error. We can clearly see from the three examples shown
here that the eye diagram is a simple visual diagnostic of
the suitability of a detector for the BiFocals measurement
system.
While our eye diagrams give a good indication that the
hardware foundation for our BiFocals instrumentation is
functioning properly, they do not serve to provide a comprehensive verification of our entire software post-processing
stack. For that, we utilize the 32-bit CRC field that is embedded in the ethernet frame. In the final software module
of our stack, as we extract and parse all the headers and

Figure 6: Eye diagrams constructed by BiFocals
software by alignment bits to statistically reconstruct shape of Unit of Information (UI), used here
to verify function of oscilloscope and photodetector:
analysis of eye provides measure of signal noise, timing jitter, and other distortions (such as InterSymbol Interference, or ISI) in measurement toolchain.

payloads of the packets in our captured trace, we remove
this CRC value and verify that it matches the payload of its
associated ethernet frame [37]. That verification effectively
closes the loop and, modulo infrequent hash collisions in the
CRC algorithm, ensures that the entirety of both hardware
and software in our BiFocals instrumentation is functioning
reliably, and thus enabling reproducible results. Across a
span of more than 50 billion analog samples presented in
Section 4, we never found a single CRC failure with our
BiFocals instrumentation.

B.

ERROR ANALYSIS

While the above verification of the BiFocals instrumentation serves a critical function in ensuring reproducible measurements, it provides only half the story. The second half
involves a careful error analysis of our timing measurements,
in which we quantify the propagation of timing uncertainty
in hardware into the error of our final reported timings of
packets.
We must primarily consider three components for timing
uncertainty in our hardware instrumentation: jitter from
the fiber components, jitter from the photodetector, and
timebase error from the oscilloscope.
Timing jitter in the passive fiber-optic components, the
splitter and the fiber-optic link, arises from thermal fluctuations. We can make a back-of-the-envelope estimation of this
jitter: the time of flight for photons in these passive optical
components tf o = l/c, and the uncertainty σt = α tf o , where
α ≈ 10 ppm from thermal expansion. With a fiber-optic
path length of l ∼ 1 m, and the speed of light in the fiber
optic dielectric as c ∼ 2 × 108 m/s. Thus, σt ≈ 1 × 10−15 s
∼ 1 fs. Further, since thermal fluctuations are relatively
slow, much less than kHz rate, it is even less likely to significantly affect our measurements, since our captured traces
have time periods no greater than a few milliseconds (limitation imposed by memory depth of modern oscilloscopes).
Characterization of the error in the photodetector and
the electrical microwave guide to the oscilloscope front-end
is more complicated and we currently do not have a way
to easily estimate its contribution to timing jitter. By testing the optical detection system with an optical source with
known (and reliable) repetition frequency, we can characterize its timing behavior. By measuring this calibrated source
with our detection chain on a sampling oscilloscope, we can
put an upper bound of 9 ps on timing jitter caused by the
photodetector and electrical microwave guide. In actual fact,
this measured timing jitter is a combination of the timing
jitter from the hardware, the source under test, and the oscilloscope’s jitter floor.
The third source of error in timing arises from the oscilloscope, the source of our hardware clock. Timebase error
in the oscilloscope is strictly a function of the clock accuracy associated with the sampling rate. Typical modern high-performance digital scope with high sampling rates
claim time measurement accuracies of σδt = 1 ppm (and,
often, 1 ppm per year for aging). Thus, for measurement
of time intervals between adjacent packets, we can expect
σtinterval = σδt × tinterval ≈ 10−6 × 6 ns ≈ 6 femtoseconds!
Obviously, this has no effect whatsoever on the uncertainty
of our measurements; we can consider the oscilloscope clock
to be perfect.
Finally, we argue that little semantic value can be attached to further quantifying the timing error for values

much smaller than the basic UI (96.970 ps) of the 10GBaseR data transmission, unless we are interested in examining
the specifics of the pulse response of an individual or an
ensemble of bits. In other words, for the sake of characterizing packet timings, the basic quanta of information is the
bit. Thus, we have quantified the error in BiFocals measurements as less than ten picoseconds, shown that this is
significantly less than the UI time in the IEEE 802.3ae standard, and therefore conclude that our measurements represent a “ground truth” reference for network packet timings.
With this understanding, we can report our results, in Section 4, with improved confidence in the reproducibility and
the reliability of measurements conducted using BiFocals.

C.

OSCILLOSCOPE SELECTION

We provide here a prescription for easy and knowledgeable selection of appropriate oscilloscope hardware, to assist
fellow researchers who wish to implement BiFocals for their
own research. BiFocals has been tested with oscilloscopes
from LeCroy (SDA 11k and WM 816Zi), Tektronix (DPO
72004B), and Agilent (DSO 91304A); in combination with
the eye diagrams of Appendix A, these guidelines should
ensure an appropriate timebase.
Oscilloscope selection is primarily based upon having adequate bandwidth and sample rate for the measurement
for the specific IEEE ethernet standard under investigation.
IEEE 802.3 standards [2] list both the nominal symbol rates
(data rates plus line code overhead) and the receive cutoff
frequency for each transmission format. In addition to oscilloscope bandwidth, perhaps an even more important metric
is the sampling rate of a given digital oscilloscope. According to the Nyquist theorem, to ensure signal fidelity, we must
sample at a rate at least twice that of the frequency component we are measuring. This translates to sampling rates on
the order of 3-5 GSa/sec for characterizing 802.3z systems
and 25-40 GSa/sec for 802.3ae systems.
Another crucial requirement for the oscilloscope is its
memory depth. Scope memory is a precious commodity
in these instruments, as it must support write rates equivalent to the fast sampling rates of modern digital scopes,
up to tens of GigaSamples per second (tens of GigaBytes
per second for 8-bit scope ADC’s)! The memory depth
of these scopes determines the length of time that can be
captured in a single acquisition at a given sampling rate.
For example, the LeCroy oscilloscope used in Section 4 features a sampling rate of 40 GSa/sec and a memory depth
of 100 MSa, enabling capture of 2.5 ms of wall-clock time
in memory. Modern (2009) high-performance scopes generally have maximum memory depths of between 250 and
1000 MSamples — equivalent to up to 25 milliseconds of
wall-clock time when measuring 10 Gbps IEEE 802.3ae (assuming 40 GSa/sec sampling rate) and 200 milliseconds of
wall clock time when measuring 1 Gbps IEEE 802.3z (assuming 5GSa/sec sampling rate).

